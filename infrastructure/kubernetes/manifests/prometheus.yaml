apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  labels:
    app: prometheus
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
      external_labels:
        cluster: 'rideshare-simulation'
        environment: 'local'

    rule_files:
      - '/etc/prometheus/rules/*.yml'

    scrape_configs:
      # Prometheus self-monitoring
      - job_name: 'prometheus'
        static_configs:
          - targets: ['localhost:9090']
            labels:
              service: 'prometheus'

      # cAdvisor container metrics (scraped directly, not via OTel)
      - job_name: 'cadvisor'
        scrape_interval: 15s
        static_configs:
          - targets: ['cadvisor:8080']
            labels:
              service: 'cadvisor'

      # OTel Collector self-metrics
      - job_name: 'otel-collector'
        scrape_interval: 15s
        static_configs:
          - targets: ['otel-collector:8888']
            labels:
              service: 'otel-collector'

      # NOTE: simulation and stream-processor no longer scraped directly.
      # They export metrics via OTLP to OTel Collector, which pushes via remote_write.

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-rules
  labels:
    app: prometheus
data:
  alerts.yml: |
    groups:
      - name: prometheus_health
        interval: 30s
        rules:
          - alert: PrometheusDown
            expr: up{job="prometheus"} == 0
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: "Prometheus instance down"
              description: "Prometheus has been down for more than 1 minute"

          - alert: PrometheusScrapeFailure
            expr: up == 0
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Target scrape failure"
              description: "Prometheus failed to scrape {{ $labels.job }} for 5 minutes"

      - name: container_health
        interval: 30s
        rules:
          - alert: HighContainerMemoryUsage
            expr: (container_memory_usage_bytes / container_spec_memory_limit_bytes) > 0.9
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "High container memory usage"
              description: "Container {{ $labels.name }} is using {{ $value | humanizePercentage }} of memory limit"

          - alert: ContainerDown
            expr: up{job="cadvisor"} == 0
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: "cAdvisor down"
              description: "Container metrics collection is unavailable"

      - name: simulation_health
        interval: 30s
        rules:
          - alert: SimulationHighErrorRate
            expr: sum(rate(simulation_errors_total[5m])) > 10
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: "High simulation error rate"
              description: "Simulation is experiencing {{ $value | humanize }} errors per second"

          - alert: SimulationOSRMLatencyHigh
            expr: histogram_quantile(0.95, rate(simulation_osrm_latency_seconds_bucket[5m])) > 1
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "High OSRM latency"
              description: "OSRM p95 latency is {{ $value | humanizeDuration }}"

          - alert: StreamProcessorKafkaDisconnected
            expr: stream_processor_kafka_connected == 0
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: "Stream processor lost Kafka connection"
              description: "Stream processor has been disconnected from Kafka for more than 1 minute"

          - alert: StreamProcessorRedisDisconnected
            expr: stream_processor_redis_connected == 0
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: "Stream processor lost Redis connection"
              description: "Stream processor has been disconnected from Redis for more than 1 minute"

          - alert: SimulationQueueBacklog
            expr: simulation_simpy_events > 1000
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "High SimPy event queue depth"
              description: "SimPy event queue has {{ $value }} pending events"

          - alert: SimulationServiceDown
            expr: up{job="simulation"} == 0
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: "Simulation service down"
              description: "Simulation service has been unreachable for more than 1 minute"

          - alert: StreamProcessorServiceDown
            expr: up{job="stream-processor"} == 0
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: "Stream processor down"
              description: "Stream processor service has been unreachable for more than 1 minute"

  performance.yml: |
    groups:
      - name: rideshare_performance
        interval: 4s
        rules:
          # --- Individual headroom components (0-1, higher = healthier) ---

          - record: rideshare:performance:kafka_lag_headroom
            expr: clamp(1 - (sum(kafka_consumergroup_lag) or vector(0)) / 18758, 0, 1)

          - record: rideshare:performance:simpy_queue_headroom
            expr: clamp(1 - (max(simulation_simpy_events) or vector(0)) / 24096, 0, 1)

          - record: rideshare:performance:cpu_headroom
            expr: >-
              clamp(
                2 * (1 - (sum by() (rate(container_cpu_usage_seconds_total{name=~"rideshare.*"}[1m]) * 100) or vector(0))
                / clamp_min(max(machine_cpu_cores) * 100, 100)),
                0, 1
              )

          - record: rideshare:performance:memory_headroom
            expr: >-
              clamp(
                1 - clamp_min((max by() (container_memory_working_set_bytes{name=~"rideshare.*"}
                  / (container_spec_memory_limit_bytes{name=~"rideshare.*"} > 0)) or vector(0)) - 0.67, 0) / 0.33,
                0, 1
              )

          - record: rideshare:performance:consumption_ratio
            expr: >-
              clamp(
                ((sum(rate(stream_processor_messages_consumed_total[1m])) or vector(0)) + 1)
                / ((sum(rate(simulation_events_total[1m])) or vector(0)) + 1),
                0, 1
              )

          - record: rideshare:performance:rtr_headroom
            expr: >-
              (
                clamp((max(simulation_real_time_ratio) - 0.66) / 0.34, 0, 1)
                and on() (sum(rate(simulation_events_total[2m])) > 0)
              )
              or vector(1)

          # --- Smoothed components (16s rolling average, ~4 samples at 4s interval) ---

          - record: rideshare:performance:kafka_lag_headroom:smooth
            expr: avg_over_time(rideshare:performance:kafka_lag_headroom[16s])

          - record: rideshare:performance:simpy_queue_headroom:smooth
            expr: avg_over_time(rideshare:performance:simpy_queue_headroom[16s])

          - record: rideshare:performance:cpu_headroom:smooth
            expr: avg_over_time(rideshare:performance:cpu_headroom[16s])

          - record: rideshare:performance:memory_headroom:smooth
            expr: avg_over_time(rideshare:performance:memory_headroom[16s])

          - record: rideshare:performance:consumption_ratio:smooth
            expr: avg_over_time(rideshare:performance:consumption_ratio[16s])

          - record: rideshare:performance:rtr_headroom:smooth
            expr: avg_over_time(rideshare:performance:rtr_headroom[16s])

          # --- Composite infrastructure headroom = min of all smoothed components ---
          - record: rideshare:infrastructure:headroom
            expr: >-
              min by() (
                label_replace(rideshare:performance:kafka_lag_headroom:smooth, "component", "kafka_lag", "", "")
                or label_replace(rideshare:performance:simpy_queue_headroom:smooth, "component", "simpy_queue", "", "")
                or label_replace(rideshare:performance:cpu_headroom:smooth, "component", "cpu", "", "")
                or label_replace(rideshare:performance:memory_headroom:smooth, "component", "memory", "", "")
                or label_replace(rideshare:performance:consumption_ratio:smooth, "component", "consumption", "", "")
                or label_replace(rideshare:performance:rtr_headroom:smooth, "component", "rtr", "", "")
              )

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus
  labels:
    app: prometheus
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      containers:
      - name: prometheus
        image: prom/prometheus:v3.9.1
        args:
        - --config.file=/etc/prometheus/prometheus.yml
        - --storage.tsdb.path=/prometheus
        - --storage.tsdb.retention.time=7d
        - --web.console.libraries=/usr/share/prometheus/console_libraries
        - --web.console.templates=/usr/share/prometheus/consoles
        - --web.enable-lifecycle
        - --web.enable-remote-write-receiver
        resources:
          limits:
            memory: "512Mi"
          requests:
            memory: "128Mi"
        ports:
        - containerPort: 9090
          name: web
        livenessProbe:
          httpGet:
            path: /-/healthy
            port: 9090
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 10
        readinessProbe:
          httpGet:
            path: /-/ready
            port: 9090
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 10
        volumeMounts:
        - name: config
          mountPath: /etc/prometheus
        - name: rules
          mountPath: /etc/prometheus/rules
        - name: data
          mountPath: /prometheus
      volumes:
      - name: config
        configMap:
          name: prometheus-config
      - name: rules
        configMap:
          name: prometheus-rules
      - name: data
        emptyDir: {}

---
apiVersion: v1
kind: Service
metadata:
  name: prometheus
  labels:
    app: prometheus
spec:
  type: ClusterIP
  ports:
  - port: 9090
    targetPort: 9090
    name: web
  selector:
    app: prometheus
