apiVersion: apps/v1
kind: Deployment
metadata:
  name: bronze-ingestion-high-volume
  labels:
    app: bronze-ingestion-high-volume
spec:
  replicas: 1
  selector:
    matchLabels:
      app: bronze-ingestion-high-volume
  template:
    metadata:
      labels:
        app: bronze-ingestion-high-volume
    spec:
      initContainers:
      - name: wait-for-kafka
        image: confluentinc/cp-kafka:7.5.0
        command:
        - sh
        - -c
        - |
          echo "Waiting for Kafka to be ready..."
          until kafka-broker-api-versions --bootstrap-server kafka-0.kafka.default.svc.cluster.local:29092 2>/dev/null; do
            echo "Kafka not ready, waiting..."
            sleep 5
          done
          echo "Kafka is ready!"
      - name: wait-for-minio
        image: busybox:1.36
        command:
        - sh
        - -c
        - |
          until nc -z minio 9000; do
            echo "Waiting for MinIO..."
            sleep 2
          done
          echo "MinIO is ready"
      - name: wait-for-hive-metastore
        image: busybox:1.36
        command:
        - sh
        - -c
        - |
          until nc -z hive-metastore 9083; do
            echo "Waiting for Hive Metastore..."
            sleep 5
          done
          echo "Hive Metastore is ready"
      containers:
      - name: bronze-ingestion-high-volume
        image: rideshare-spark-streaming:local
        imagePullPolicy: Never
        command:
        - /opt/spark/bin/spark-submit
        - --master
        - local[2]
        - --driver-memory
        - 1536m
        - --conf
        - spark.driver.extraJavaOptions=-XX:+UseG1GC -XX:MaxGCPauseMillis=50 -XX:InitiatingHeapOccupancyPercent=35
        - --name
        - streaming_high_volume
        - --conf
        - spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
        - --conf
        - spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog
        - --conf
        - spark.driver.memoryOverhead=256m
        - --conf
        - spark.driver.maxResultSize=256m
        - --conf
        - spark.ui.retainedStages=100
        - --conf
        - spark.ui.retainedJobs=100
        - --conf
        - spark.ui.retainedDeadExecutors=10
        - --conf
        - spark.sql.codegen.cache.maxEntries=100
        - --conf
        - spark.hadoop.fs.s3a.endpoint=http://minio:9000
        - --conf
        - spark.hadoop.fs.s3a.access.key=minioadmin
        - --conf
        - spark.hadoop.fs.s3a.secret.key=minioadmin
        - --conf
        - spark.hadoop.fs.s3a.path.style.access=true
        - --conf
        - spark.hadoop.fs.s3a.threads.max=8
        - --conf
        - spark.hadoop.fs.s3a.connection.pool.size=8
        - /opt/spark_streaming/jobs/bronze_ingestion_high_volume.py
        resources:
          limits:
            memory: "2Gi"
          requests:
            memory: "1Gi"
            cpu: "250m"
        env:
        - name: PYTHONPATH
          value: /opt
        - name: KAFKA_BOOTSTRAP_SERVERS
          value: kafka-0.kafka.default.svc.cluster.local:29092
        - name: SCHEMA_REGISTRY_URL
          value: http://schema-registry:8081
        - name: KAFKA_TOPICS
          value: gps_pings
        - name: CHECKPOINT_BASE_PATH
          value: s3a://rideshare-checkpoints/
        - name: TRIGGER_INTERVAL
          value: 10 seconds
        livenessProbe:
          exec:
            command:
            - sh
            - -c
            - pgrep -f 'spark-submit' || exit 1
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
        readinessProbe:
          exec:
            command:
            - sh
            - -c
            - pgrep -f 'spark-submit' || exit 1
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 10

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: bronze-ingestion-low-volume
  labels:
    app: bronze-ingestion-low-volume
spec:
  replicas: 1
  selector:
    matchLabels:
      app: bronze-ingestion-low-volume
  template:
    metadata:
      labels:
        app: bronze-ingestion-low-volume
    spec:
      initContainers:
      - name: wait-for-kafka
        image: confluentinc/cp-kafka:7.5.0
        command:
        - sh
        - -c
        - |
          echo "Waiting for Kafka to be ready..."
          until kafka-broker-api-versions --bootstrap-server kafka-0.kafka.default.svc.cluster.local:29092 2>/dev/null; do
            echo "Kafka not ready, waiting..."
            sleep 5
          done
          echo "Kafka is ready!"
      - name: wait-for-minio
        image: busybox:1.36
        command:
        - sh
        - -c
        - |
          until nc -z minio 9000; do
            echo "Waiting for MinIO..."
            sleep 2
          done
          echo "MinIO is ready"
      - name: wait-for-hive-metastore
        image: busybox:1.36
        command:
        - sh
        - -c
        - |
          until nc -z hive-metastore 9083; do
            echo "Waiting for Hive Metastore..."
            sleep 5
          done
          echo "Hive Metastore is ready"
      containers:
      - name: bronze-ingestion-low-volume
        image: rideshare-spark-streaming:local
        imagePullPolicy: Never
        command:
        - /opt/spark/bin/spark-submit
        - --master
        - local[2]
        - --driver-memory
        - 1536m
        - --conf
        - spark.driver.extraJavaOptions=-XX:+UseG1GC -XX:MaxGCPauseMillis=50 -XX:InitiatingHeapOccupancyPercent=35
        - --name
        - streaming_low_volume
        - --conf
        - spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
        - --conf
        - spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog
        - --conf
        - spark.driver.memoryOverhead=256m
        - --conf
        - spark.driver.maxResultSize=256m
        - --conf
        - spark.ui.retainedStages=100
        - --conf
        - spark.ui.retainedJobs=100
        - --conf
        - spark.ui.retainedDeadExecutors=10
        - --conf
        - spark.sql.codegen.cache.maxEntries=100
        - --conf
        - spark.hadoop.fs.s3a.endpoint=http://minio:9000
        - --conf
        - spark.hadoop.fs.s3a.access.key=minioadmin
        - --conf
        - spark.hadoop.fs.s3a.secret.key=minioadmin
        - --conf
        - spark.hadoop.fs.s3a.path.style.access=true
        - --conf
        - spark.hadoop.fs.s3a.threads.max=8
        - --conf
        - spark.hadoop.fs.s3a.connection.pool.size=8
        - /opt/spark_streaming/jobs/bronze_ingestion_low_volume.py
        resources:
          limits:
            memory: "2Gi"
          requests:
            memory: "1Gi"
            cpu: "250m"
        env:
        - name: PYTHONPATH
          value: /opt
        - name: KAFKA_BOOTSTRAP_SERVERS
          value: kafka-0.kafka.default.svc.cluster.local:29092
        - name: SCHEMA_REGISTRY_URL
          value: http://schema-registry:8081
        - name: KAFKA_TOPICS
          value: trips,driver_status,surge_updates,ratings,payments,driver_profiles,rider_profiles
        - name: CHECKPOINT_BASE_PATH
          value: s3a://rideshare-checkpoints/
        - name: TRIGGER_INTERVAL
          value: 30 seconds
        livenessProbe:
          exec:
            command:
            - sh
            - -c
            - pgrep -f 'spark-submit' || exit 1
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
        readinessProbe:
          exec:
            command:
            - sh
            - -c
            - pgrep -f 'spark-submit' || exit 1
          initialDelaySeconds: 30
          periodSeconds: 30
          timeoutSeconds: 10
