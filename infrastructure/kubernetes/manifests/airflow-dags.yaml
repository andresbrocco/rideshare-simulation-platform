apiVersion: v1
kind: ConfigMap
metadata:
  name: airflow-dags
  labels:
    app: airflow
    component: dags
data:
  dbt_transformation_dag.py: |
    """DBT transformation DAGs for Silver and Gold layers."""

    import os
    from datetime import datetime, timedelta
    from airflow import DAG
    from airflow.providers.standard.operators.bash import BashOperator
    from airflow.providers.standard.operators.python import (
        BranchPythonOperator,
        ShortCircuitOperator,
    )
    from airflow.providers.standard.operators.empty import EmptyOperator
    from airflow.providers.standard.operators.trigger_dagrun import TriggerDagRunOperator
    from airflow.sdk import Asset

    # Asset definitions for data lineage
    SILVER_ASSET = Asset("lakehouse://silver/transformed")
    GOLD_ASSET = Asset("lakehouse://gold/transformed")

    # MinIO / S3 configuration for Bronze table checks
    MINIO_ENDPOINT = os.environ.get("AWS_ENDPOINT_URL")
    BRONZE_BUCKET = os.environ.get("BRONZE_BUCKET", "rideshare-bronze")

    STORAGE_OPTIONS: dict[str, str] = {
        "AWS_S3_ALLOW_UNSAFE_RENAME": "true",
    }
    if MINIO_ENDPOINT:
        # Local dev: explicit MinIO credentials
        STORAGE_OPTIONS["AWS_ACCESS_KEY_ID"] = os.environ.get("AWS_ACCESS_KEY_ID", "minioadmin")
        STORAGE_OPTIONS["AWS_SECRET_ACCESS_KEY"] = os.environ.get("AWS_SECRET_ACCESS_KEY", "minioadmin")
        STORAGE_OPTIONS["AWS_ENDPOINT_URL"] = MINIO_ENDPOINT
        STORAGE_OPTIONS["AWS_ALLOW_HTTP"] = "true" if MINIO_ENDPOINT.startswith("http://") else "false"
    else:
        # Production: use IRSA/Pod Identity credential chain
        STORAGE_OPTIONS["AWS_REGION"] = os.environ.get("AWS_REGION", "us-east-1")

    # Bronze tables required for Silver layer transformations
    REQUIRED_BRONZE_TABLES = [
        "bronze_trips",
        "bronze_gps_pings",
        "bronze_driver_status",
        "bronze_surge_updates",
        "bronze_ratings",
        "bronze_payments",
        "bronze_driver_profiles",
        "bronze_rider_profiles",
    ]


    def check_bronze_data_exists() -> bool:
        """Check whether all required Bronze Delta tables exist.

        In local dev (AWS_ENDPOINT_URL set), pings MinIO health endpoint first.
        In production (AWS_ENDPOINT_URL empty), skips health check and goes
        straight to Delta table existence checks.
        Returns True to proceed with the DAG, False to skip all downstream tasks.
        """
        from deltalake import DeltaTable

        if MINIO_ENDPOINT:
            import urllib.request

            req = urllib.request.Request(f"{MINIO_ENDPOINT}/minio/health/live", method="GET")
            with urllib.request.urlopen(req, timeout=5):
                pass
            print(f"Connected to MinIO at {MINIO_ENDPOINT}")
        else:
            print("Production mode — skipping MinIO health check")

        for table in REQUIRED_BRONZE_TABLES:
            path = f"s3://{BRONZE_BUCKET}/{table}/"
            if not DeltaTable.is_deltatable(path, storage_options=STORAGE_OPTIONS):
                print(f"Bronze table missing: {table}")
                print("Skipping DAG run — Bronze data not ready yet")
                return False

        print("All Bronze tables present — proceeding with transformations")
        return True


    default_args = {
        "owner": "rideshare",
        "depends_on_past": False,
        "email_on_failure": False,
        "email_on_retry": False,
        "retries": 2,
        "retry_delay": timedelta(minutes=5),
    }

    SILVER_SCHEDULE = os.environ.get("SILVER_SCHEDULE", "10 * * * *")

    # Silver DAG - Runs at :10 past each hour with Bronze freshness check (configurable via SILVER_SCHEDULE env var)
    with DAG(
        "dbt_silver_transformation",
        default_args=default_args,
        description="DBT Silver layer transformations",
        schedule=SILVER_SCHEDULE,
        start_date=datetime(2026, 1, 1),
        catchup=False,
        max_active_runs=1,
        tags=["dbt", "silver", "transformation"],
    ) as silver_dag:

        check_bronze_freshness = ShortCircuitOperator(
            task_id="check_bronze_freshness",
            python_callable=check_bronze_data_exists,
        )

        dbt_silver_run = BashOperator(
            task_id="dbt_silver_run",
            bash_command="cd /opt/dbt/dbt && dbt run --select tag:silver --target local --profiles-dir /opt/dbt",
        )

        dbt_silver_test = BashOperator(
            task_id="dbt_silver_test",
            bash_command="cd /opt/dbt/dbt && dbt test --select tag:silver --target local --threads 2 --profiles-dir /opt/dbt",
        )

        ge_silver_validation = BashOperator(
            task_id="ge_silver_validation",
            bash_command="""
            cd /opt/great-expectations && \
            python3 run_checkpoint.py silver_validation || echo "WARNING: Silver validation failed" && exit 0
            """,
            outlets=[SILVER_ASSET],
        )

        export_silver_to_s3 = BashOperator(
            task_id="export_silver_to_s3",
            bash_command="python3 /opt/init-scripts/export-dbt-to-s3.py --layer silver",
        )

        register_silver_trino_tables = BashOperator(
            task_id="register_silver_trino_tables",
            bash_command="python3 /opt/init-scripts/register-trino-tables.py --layer silver",
        )

        def should_trigger_gold(**context) -> str:
            """Trigger Gold DAG at 2 AM, for manual runs, or when not in PROD_MODE."""
            prod_mode = os.environ.get("PROD_MODE", "false").lower() == "true"
            logical_date = context.get("logical_date")
            # Trigger if: not PROD_MODE, manual run (no logical_date), or scheduled at 2 AM
            if not prod_mode or logical_date is None or logical_date.hour == 2:
                return "trigger_gold_dag"
            return "skip_gold_trigger"

        check_should_trigger_gold = BranchPythonOperator(
            task_id="check_should_trigger_gold",
            python_callable=should_trigger_gold,
        )

        trigger_gold_dag = TriggerDagRunOperator(
            task_id="trigger_gold_dag",
            trigger_dag_id="dbt_gold_transformation",
            wait_for_completion=False,
            conf={
                "triggered_by": "silver_dag",
                "silver_logical_date": "{{ logical_date | default('manual', true) }}",
            },
        )

        skip_gold_trigger = EmptyOperator(task_id="skip_gold_trigger")

        # Task dependencies
        (
            check_bronze_freshness
            >> dbt_silver_run
            >> dbt_silver_test
            >> ge_silver_validation
            >> export_silver_to_s3
        )
        (
            export_silver_to_s3
            >> register_silver_trino_tables
            >> check_should_trigger_gold
            >> [trigger_gold_dag, skip_gold_trigger]
        )

    # Gold DAG - Triggered by Silver DAG at 2 AM (no schedule)
    with DAG(
        "dbt_gold_transformation",
        default_args=default_args,
        description="DBT Gold layer transformations (triggered by Silver at 2 AM)",
        schedule=None,
        start_date=datetime(2026, 1, 1),
        catchup=False,
        max_active_runs=1,
        tags=["dbt", "gold", "transformation"],
    ) as gold_dag:

        dbt_seed = BashOperator(
            task_id="dbt_seed",
            bash_command="cd /opt/dbt/dbt && dbt seed --target local --profiles-dir /opt/dbt",
        )

        dbt_gold_dimensions = BashOperator(
            task_id="dbt_gold_dimensions",
            bash_command="cd /opt/dbt/dbt && dbt run --select tag:dimensions --target local --profiles-dir /opt/dbt",
        )

        dbt_gold_facts = BashOperator(
            task_id="dbt_gold_facts",
            bash_command="cd /opt/dbt/dbt && dbt run --select tag:facts --target local --profiles-dir /opt/dbt",
        )

        dbt_gold_aggregates = BashOperator(
            task_id="dbt_gold_aggregates",
            bash_command="cd /opt/dbt/dbt && dbt run --select tag:aggregates --target local --profiles-dir /opt/dbt",
        )

        dbt_gold_test = BashOperator(
            task_id="dbt_gold_test",
            bash_command="cd /opt/dbt/dbt && dbt test --select tag:gold --target local --threads 2 --profiles-dir /opt/dbt",
        )

        ge_gold_validation = BashOperator(
            task_id="ge_gold_validation",
            bash_command="""
            cd /opt/great-expectations && \
            python3 run_checkpoint.py gold_validation || echo "WARNING: Gold validation failed" && exit 0
            """,
        )

        export_gold_to_s3 = BashOperator(
            task_id="export_gold_to_s3",
            bash_command="python3 /opt/init-scripts/export-dbt-to-s3.py --layer gold",
        )

        register_gold_trino_tables = BashOperator(
            task_id="register_gold_trino_tables",
            bash_command="python3 /opt/init-scripts/register-trino-tables.py --layer gold",
        )

        ge_generate_data_docs = BashOperator(
            task_id="ge_generate_data_docs",
            bash_command="""
            cd /opt/great-expectations && \
            python3 build_data_docs.py
            """,
            outlets=[GOLD_ASSET],
        )

        (
            dbt_seed
            >> dbt_gold_dimensions
            >> dbt_gold_facts
            >> dbt_gold_aggregates
            >> dbt_gold_test
            >> ge_gold_validation
            >> export_gold_to_s3
            >> register_gold_trino_tables
            >> ge_generate_data_docs
        )
  dlq_monitoring_dag.py: |
    """DLQ monitoring DAG that checks error counts every 15 minutes.

    Connects to MinIO via DuckDB to query DLQ Delta tables for recent errors.
    """

    import os
    from datetime import datetime, timedelta
    from airflow import DAG
    from airflow.providers.standard.operators.python import (
        PythonOperator,
        BranchPythonOperator,
    )
    from airflow.providers.standard.operators.empty import EmptyOperator

    ERROR_THRESHOLD = 10
    MINIO_ENDPOINT = os.environ.get("AWS_ENDPOINT_URL")
    BRONZE_BUCKET = os.environ.get("BRONZE_BUCKET", "rideshare-bronze")

    DLQ_TABLES = [
        "dlq_bronze_trips",
        "dlq_bronze_gps_pings",
        "dlq_bronze_driver_status",
        "dlq_bronze_surge_updates",
        "dlq_bronze_ratings",
        "dlq_bronze_payments",
        "dlq_bronze_driver_profiles",
        "dlq_bronze_rider_profiles",
    ]

    default_args = {
        "owner": "rideshare",
        "depends_on_past": False,
        "email_on_failure": False,
        "email_on_retry": False,
        "retries": 2,
        "retry_delay": timedelta(minutes=5),
    }


    def query_dlq_errors(**context):
        """Query all DLQ tables for error counts in the last 15 minutes.

        Uses DuckDB with delta and httpfs extensions to read Delta tables
        directly from MinIO without requiring Spark.
        """
        import duckdb
        from datetime import datetime, timedelta

        error_counts = {}
        total_errors = 0
        cutoff_time = (datetime.now() - timedelta(minutes=15)).strftime("%Y-%m-%d %H:%M:%S")

        try:
            conn = duckdb.connect(":memory:")
            conn.install_extension("delta")
            conn.install_extension("httpfs")
            conn.load_extension("delta")
            conn.load_extension("httpfs")

            if MINIO_ENDPOINT:
                # Local dev: point DuckDB at MinIO with static credentials
                access_key = os.environ.get("AWS_ACCESS_KEY_ID", "minioadmin")
                secret_key = os.environ.get("AWS_SECRET_ACCESS_KEY", "minioadmin")
                endpoint_host = MINIO_ENDPOINT.replace("http://", "").replace("https://", "")
                use_ssl = "false" if MINIO_ENDPOINT.startswith("http://") else "true"
                conn.execute(
                    f"""
                    CREATE SECRET minio_secret (
                        TYPE S3,
                        KEY_ID '{access_key}',
                        SECRET '{secret_key}',
                        ENDPOINT '{endpoint_host}',
                        USE_SSL {use_ssl},
                        URL_STYLE 'path'
                    )
                """
                )
            else:
                # Production: use IRSA/Pod Identity credential chain
                region = os.environ.get("AWS_REGION", "us-east-1")
                conn.execute(
                    f"""
                    CREATE SECRET aws_secret (
                        TYPE S3,
                        PROVIDER CREDENTIAL_CHAIN,
                        REGION '{region}'
                    )
                """
                )

            for table in DLQ_TABLES:
                try:
                    query = f"""
                        SELECT COUNT(*) as cnt
                        FROM delta_scan('s3://{BRONZE_BUCKET}/{table}/')
                        WHERE _ingested_at >= '{cutoff_time}'
                    """
                    result = conn.execute(query).fetchone()
                    count = result[0] if result else 0
                    error_counts[table] = count
                    total_errors += count

                    print(f"DLQ table {table}: {count} errors in last 15 minutes")
                except Exception as e:
                    print(f"Warning: Could not query {table}: {e}")
                    error_counts[table] = 0

            conn.close()
        except Exception as e:
            print(f"Error connecting to DuckDB: {e}")
            print("DLQ tables may not exist yet - this is expected on first run")
            for table in DLQ_TABLES:
                error_counts[table] = 0

        context["ti"].xcom_push(key="error_counts", value=error_counts)
        context["ti"].xcom_push(key="total_errors", value=total_errors)

        print(f"Total DLQ errors: {total_errors}")
        return error_counts


    def check_threshold(**context):
        """Check if error count exceeds threshold and branch accordingly."""
        total_errors = context["ti"].xcom_pull(task_ids="query_dlq_errors", key="total_errors")

        if total_errors is None:
            total_errors = 0

        print(f"Checking threshold: {total_errors} errors vs {ERROR_THRESHOLD} threshold")

        if total_errors > ERROR_THRESHOLD:
            return "send_alert"
        return "no_alert"


    def send_alert(**context):
        """Send alert when error threshold is exceeded."""
        total_errors = context["ti"].xcom_pull(task_ids="query_dlq_errors", key="total_errors")
        error_counts = context["ti"].xcom_pull(task_ids="query_dlq_errors", key="error_counts")

        print("ALERT: DLQ errors exceeded threshold!")
        print(f"Total errors: {total_errors}")
        print("Error breakdown by table:")

        for table, count in error_counts.items():
            if count > 0:
                print(f"  - {table}: {count} errors")


    with DAG(
        "dlq_monitoring",
        default_args=default_args,
        description="Monitor DLQ tables for errors every 15 minutes",
        schedule="3,18,33,48 * * * *",
        start_date=datetime(2026, 1, 1),
        catchup=False,
        tags=["monitoring", "dlq"],
    ) as dag:

        query_dlq_errors_task = PythonOperator(
            task_id="query_dlq_errors",
            python_callable=query_dlq_errors,
        )

        check_threshold_task = BranchPythonOperator(
            task_id="check_threshold",
            python_callable=check_threshold,
        )

        send_alert_task = PythonOperator(
            task_id="send_alert",
            python_callable=send_alert,
        )

        no_alert_task = EmptyOperator(
            task_id="no_alert",
        )

        query_dlq_errors_task >> check_threshold_task >> [send_alert_task, no_alert_task]
  delta_maintenance_dag.py: |
    """Delta Lake maintenance DAG for OPTIMIZE and VACUUM operations.

    This DAG performs daily maintenance on Bronze Delta tables to:
    1. OPTIMIZE: Compact small files into larger ones for better read performance
    2. VACUUM: Remove old files no longer referenced by the Delta log

    Uses delta-rs Python library to perform operations directly on Delta tables
    stored in MinIO (S3-compatible) without requiring Spark.

    Schedule: 3 AM daily (after Gold DAG completes at 2 AM)
    """

    import os
    from datetime import datetime, timedelta
    from typing import Any

    from airflow import DAG
    from airflow.providers.standard.operators.python import PythonOperator, ShortCircuitOperator
    from airflow.providers.standard.operators.empty import EmptyOperator

    # Bronze tables to maintain
    BRONZE_TABLES = [
        "bronze_trips",
        "bronze_gps_pings",
        "bronze_driver_status",
        "bronze_surge_updates",
        "bronze_ratings",
        "bronze_payments",
        "bronze_driver_profiles",
        "bronze_rider_profiles",
    ]

    # DLQ tables (one per Bronze table)
    DLQ_TABLES = [f"dlq_{t}" for t in BRONZE_TABLES]

    # All tables to maintain
    ALL_TABLES = BRONZE_TABLES + DLQ_TABLES

    # VACUUM retention in hours (7 days)
    VACUUM_RETENTION_HOURS = 168

    # S3 / MinIO configuration (read from environment for cloud portability)
    MINIO_ENDPOINT = os.environ.get("AWS_ENDPOINT_URL")
    BRONZE_BUCKET = os.environ.get("BRONZE_BUCKET", "rideshare-bronze")

    # S3 storage options for delta-rs
    STORAGE_OPTIONS: dict[str, str] = {
        "AWS_S3_ALLOW_UNSAFE_RENAME": "true",
    }
    if MINIO_ENDPOINT:
        # Local dev: explicit MinIO credentials
        STORAGE_OPTIONS["AWS_ACCESS_KEY_ID"] = os.environ.get("AWS_ACCESS_KEY_ID", "minioadmin")
        STORAGE_OPTIONS["AWS_SECRET_ACCESS_KEY"] = os.environ.get("AWS_SECRET_ACCESS_KEY", "minioadmin")
        STORAGE_OPTIONS["AWS_ENDPOINT_URL"] = MINIO_ENDPOINT
        STORAGE_OPTIONS["AWS_ALLOW_HTTP"] = "true" if MINIO_ENDPOINT.startswith("http://") else "false"
    else:
        # Production: use IRSA/Pod Identity credential chain
        STORAGE_OPTIONS["AWS_REGION"] = os.environ.get("AWS_REGION", "us-east-1")


    def check_bronze_data_exists() -> bool:
        """Check whether any Bronze Delta tables exist.

        In local dev (AWS_ENDPOINT_URL set), pings MinIO health endpoint first.
        In production (AWS_ENDPOINT_URL empty), skips health check and goes
        straight to Delta table existence checks.
        Returns True to proceed with maintenance, False to skip all downstream tasks.
        """
        from deltalake import DeltaTable

        if MINIO_ENDPOINT:
            import urllib.request

            req = urllib.request.Request(f"{MINIO_ENDPOINT}/minio/health/live", method="GET")
            with urllib.request.urlopen(req, timeout=5):
                pass
            print(f"Connected to MinIO at {MINIO_ENDPOINT}")
        else:
            print("Production mode — skipping MinIO health check")

        for table in BRONZE_TABLES:
            path = f"s3://{BRONZE_BUCKET}/{table}/"
            if DeltaTable.is_deltatable(path, storage_options=STORAGE_OPTIONS):
                print(f"Found Bronze table: {table} — proceeding with maintenance")
                return True

        print("No Bronze tables found — skipping maintenance run")
        return False


    def optimize_table(table_name: str, **context: Any) -> dict[str, object]:
        """Run OPTIMIZE on a Delta table using delta-rs.

        Compacts small files into larger ones for better read performance.

        Args:
            table_name: Name of the table to optimize.
            **context: Airflow context.

        Returns:
            Dictionary with table name and operation status.
        """
        from deltalake import DeltaTable

        try:
            dt = DeltaTable(f"s3://{BRONZE_BUCKET}/{table_name}/", storage_options=STORAGE_OPTIONS)
            metrics = dt.optimize.compact()
            return {
                "table": table_name,
                "status": "success",
                "operation": "OPTIMIZE",
                "metrics": metrics,
            }
        except Exception as e:
            return {"table": table_name, "status": "failed", "error": str(e), "operation": "OPTIMIZE"}


    def vacuum_table(
        table_name: str, retention_hours: int = VACUUM_RETENTION_HOURS, **context: Any
    ) -> dict[str, object]:
        """Run VACUUM on a Delta table using delta-rs.

        Removes old files no longer referenced by the Delta transaction log.

        Args:
            table_name: Name of the table to vacuum.
            retention_hours: Hours of history to retain.
            **context: Airflow context.

        Returns:
            Dictionary with table name and operation status.
        """
        from deltalake import DeltaTable

        try:
            dt = DeltaTable(f"s3://{BRONZE_BUCKET}/{table_name}/", storage_options=STORAGE_OPTIONS)
            metrics = dt.vacuum(
                retention_hours=retention_hours, enforce_retention_duration=False, dry_run=False
            )
            return {"table": table_name, "status": "success", "operation": "VACUUM", "metrics": metrics}
        except Exception as e:
            return {"table": table_name, "status": "failed", "error": str(e), "operation": "VACUUM"}


    def summarize_results(**context: Any) -> dict[str, Any]:
        """Summarize OPTIMIZE and VACUUM results from all tasks.

        Args:
            **context: Airflow context containing task instance.

        Returns:
            Dictionary with summary of all operations.
        """
        ti = context["ti"]
        results: dict[str, list[dict[str, object]]] = {"optimize": [], "vacuum": []}

        for table in ALL_TABLES:
            optimize_result = ti.xcom_pull(task_ids=f"optimize_{table}")
            vacuum_result = ti.xcom_pull(task_ids=f"vacuum_{table}")

            if optimize_result:
                results["optimize"].append(optimize_result)
            if vacuum_result:
                results["vacuum"].append(vacuum_result)

        # Count successes and failures
        optimize_success = sum(1 for r in results["optimize"] if r.get("status") == "success")
        optimize_failed = len(results["optimize"]) - optimize_success
        vacuum_success = sum(1 for r in results["vacuum"] if r.get("status") == "success")
        vacuum_failed = len(results["vacuum"]) - vacuum_success

        summary = {
            "total_tables": len(ALL_TABLES),
            "optimize_success": optimize_success,
            "optimize_failed": optimize_failed,
            "vacuum_success": vacuum_success,
            "vacuum_failed": vacuum_failed,
        }

        print(f"Delta Maintenance Summary: {summary}")
        return summary


    default_args = {
        "owner": "rideshare",
        "depends_on_past": False,
        "email_on_failure": False,
        "email_on_retry": False,
        "retries": 1,
        "retry_delay": timedelta(minutes=5),
    }

    with DAG(
        "delta_maintenance",
        default_args=default_args,
        description="Daily OPTIMIZE and VACUUM for Bronze Delta tables",
        schedule="0 3 * * *",  # 3 AM daily
        start_date=datetime(2026, 1, 1),
        catchup=False,
        max_active_tasks=4,  # Limit parallel operations
        tags=["delta", "maintenance", "bronze"],
    ) as dag:

        start = EmptyOperator(task_id="start")

        check_data_exists = ShortCircuitOperator(
            task_id="check_data_exists",
            python_callable=check_bronze_data_exists,
        )

        start >> check_data_exists

        # Create OPTIMIZE tasks for all tables
        optimize_tasks = []
        for table in ALL_TABLES:
            task = PythonOperator(
                task_id=f"optimize_{table}",
                python_callable=optimize_table,
                op_kwargs={"table_name": table},
            )
            check_data_exists >> task
            optimize_tasks.append(task)

        # Barrier between OPTIMIZE and VACUUM
        optimize_complete = EmptyOperator(task_id="optimize_complete")
        for task in optimize_tasks:
            task >> optimize_complete

        # Create VACUUM tasks for all tables
        vacuum_tasks = []
        for table in ALL_TABLES:
            task = PythonOperator(
                task_id=f"vacuum_{table}",
                python_callable=vacuum_table,
                op_kwargs={"table_name": table, "retention_hours": VACUUM_RETENTION_HOURS},
            )
            optimize_complete >> task
            vacuum_tasks.append(task)

        # Barrier after VACUUM
        vacuum_complete = EmptyOperator(task_id="vacuum_complete")
        for task in vacuum_tasks:
            task >> vacuum_complete

        # Summarize results
        summarize = PythonOperator(
            task_id="summarize",
            python_callable=summarize_results,
        )

        vacuum_complete >> summarize
