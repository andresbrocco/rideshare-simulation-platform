name: rideshare-platform

# Docker Compose Profiles:
# - core: Simulation runtime services (kafka, redis, osrm, simulation, stream-processor, frontend)
# - data-pipeline: ETL, ingestion, and orchestration (minio, bronze-ingestion, localstack, airflow, hive-metastore, trino)
# - spark-testing: Spark Thrift Server for dual-engine DBT validation (optional, use with data-pipeline)
# - monitoring: Observability (prometheus, cadvisor, grafana, otel-collector, loki, tempo)

services:
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: rideshare-kafka
    deploy:
      resources:
        limits:
          memory: 1g
          cpus: "1.5"
        reservations:
          cpus: "0.25"
    profiles:
      - core
      - data-pipeline
    ports:
      - "9092:9092"
    depends_on:
      secrets-init:
        condition: service_completed_successfully
    environment:
      # JVM flags for ARM/Apple Silicon stability
      KAFKA_OPTS: "-XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -Djava.security.auth.login.config=/tmp/kafka_jaas.conf"
      KAFKA_HEAP_OPTS: "-Xms256m -Xmx512m"

      # Log retention - auto-delete since no consumers yet
      KAFKA_LOG_RETENTION_HOURS: 1
      KAFKA_LOG_RETENTION_BYTES: 536870912  # 512MB per partition

      # Threading - increased for 23 partitions + 3 consumer groups
      KAFKA_NUM_NETWORK_THREADS: "6"
      KAFKA_NUM_IO_THREADS: "16"
      # Socket buffers
      KAFKA_SOCKET_SEND_BUFFER_BYTES: "102400"
      KAFKA_SOCKET_RECEIVE_BUFFER_BYTES: "102400"

      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_LISTENERS: SASL_PLAINTEXT://kafka:29092,CONTROLLER://kafka:29093,SASL_PLAINTEXT_HOST://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: SASL_PLAINTEXT://kafka:29092,SASL_PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_PLAINTEXT_HOST:SASL_PLAINTEXT
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:29093
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_INTER_BROKER_LISTENER_NAME: SASL_PLAINTEXT
      KAFKA_SASL_MECHANISM_INTER_BROKER_PROTOCOL: PLAIN
      KAFKA_SASL_ENABLED_MECHANISMS: PLAIN
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_LOG_DIRS: /tmp/kraft-combined-logs
      CLUSTER_ID: MkU3OEVBNTcwNTJENDM2Qk
    entrypoint:
      - /bin/sh
      - -c
      - |
        set -a && . /secrets/core.env && set +a
        printf 'KafkaServer {\n    org.apache.kafka.common.security.plain.PlainLoginModule required\n    username="%s"\n    password="%s"\n    user_%s="%s";\n};\n' "$$KAFKA_SASL_USERNAME" "$$KAFKA_SASL_PASSWORD" "$$KAFKA_SASL_USERNAME" "$$KAFKA_SASL_PASSWORD" > /tmp/kafka_jaas.conf
        printf 'security.protocol=SASL_PLAINTEXT\nsasl.mechanism=PLAIN\nsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="%s" password="%s";\n' "$$KAFKA_SASL_USERNAME" "$$KAFKA_SASL_PASSWORD" > /tmp/kafka-client.properties
        exec /etc/confluent/docker/run
    volumes:
      - kafka-data:/var/lib/kafka/data
      - secrets-volume:/secrets:ro
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD-SHELL", "kafka-broker-api-versions --bootstrap-server localhost:9092 --command-config /tmp/kafka-client.properties"]
      interval: 10s
      timeout: 10s
      retries: 10
    restart: unless-stopped

  kafka-init:
    image: confluentinc/cp-kafka:7.5.0
    container_name: rideshare-kafka-init
    profiles:
      - core
      - data-pipeline
    depends_on:
      secrets-init:
        condition: service_completed_successfully
      kafka:
        condition: service_healthy
    volumes:
      - ../../services/kafka/topics.yaml:/etc/kafka/topics.yaml:ro
      - ../../services/kafka/create-topics.sh:/etc/kafka/create-topics.sh:ro
      - secrets-volume:/secrets:ro
    entrypoint:
      - /bin/sh
      - -c
      - |
        set -a && . /secrets/core.env && set +a
        printf 'security.protocol=SASL_PLAINTEXT\nsasl.mechanism=PLAIN\nsasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username="%s" password="%s";\n' "$$KAFKA_SASL_USERNAME" "$$KAFKA_SASL_PASSWORD" > /tmp/kafka-client.properties
        export KAFKA_COMMAND_CONFIG=/tmp/kafka-client.properties
        exec /bin/sh /etc/kafka/create-topics.sh
    networks:
      - rideshare-network

  schema-registry:
    image: confluentinc/cp-schema-registry:7.5.0
    container_name: rideshare-schema-registry
    deploy:
      resources:
        limits:
          memory: 512m
          cpus: "0.5"
        reservations:
          cpus: "0.1"
    profiles:
      - core
      - data-pipeline
    ports:
      - "8085:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka:29092
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
      SCHEMA_REGISTRY_HEAP_OPTS: "-Xms128m -Xmx256m"
      SCHEMA_REGISTRY_KAFKASTORE_SECURITY_PROTOCOL: SASL_PLAINTEXT
      SCHEMA_REGISTRY_KAFKASTORE_SASL_MECHANISM: PLAIN
      SCHEMA_REGISTRY_AUTHENTICATION_METHOD: BASIC
      SCHEMA_REGISTRY_AUTHENTICATION_ROLES: admin
      SCHEMA_REGISTRY_AUTHENTICATION_REALM: SchemaRegistry
      SCHEMA_REGISTRY_OPTS: "-Djava.security.auth.login.config=/etc/schema-registry/jaas.conf"
    entrypoint:
      - /bin/sh
      - -c
      - |
        set -a && . /secrets/core.env && set +a
        export SCHEMA_REGISTRY_KAFKASTORE_SASL_JAAS_CONFIG=$$(printf 'org.apache.kafka.common.security.plain.PlainLoginModule required username="%s" password="%s";' "$$KAFKA_SASL_USERNAME" "$$KAFKA_SASL_PASSWORD")
        exec /etc/confluent/docker/run
    depends_on:
      secrets-init:
        condition: service_completed_successfully
      kafka-init:
        condition: service_completed_successfully
    volumes:
      - secrets-volume:/secrets:ro
      - ../../services/schema-registry/jaas.conf:/etc/schema-registry/jaas.conf:ro
      - ../../services/schema-registry/users.properties:/etc/schema-registry/users.properties:ro
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "curl", "-f", "-u", "admin:admin", "http://localhost:8081/subjects"]
      interval: 10s
      timeout: 10s
      retries: 12
      start_period: 30s
    restart: unless-stopped

  redis:
    image: redis:8.0-alpine
    container_name: rideshare-redis
    mem_limit: 128m
    profiles:
      - core
    ports:
      - "6379:6379"
    entrypoint:
      - /bin/sh
      - -c
      - |
        set -a && . /secrets/core.env && set +a
        exec redis-server --requirepass "$$REDIS_PASSWORD"
    depends_on:
      secrets-init:
        condition: service_completed_successfully
    volumes:
      - redis-data:/data
      - secrets-volume:/secrets:ro
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD-SHELL", ". /secrets/core.env && redis-cli -a \"$$REDIS_PASSWORD\" ping"]
      interval: 5s
      timeout: 3s
      retries: 5
    restart: unless-stopped

  osrm:
    build:
      context: ../../services/osrm
      dockerfile: Dockerfile
      args:
        OSRM_MAP_SOURCE: ${OSRM_MAP_SOURCE:-local}
    container_name: rideshare-osrm
    mem_limit: 1g
    profiles:
      - core
    platform: linux/amd64
    ports:
      - "5050:5000"
    environment:
      - OSRM_THREADS=${OSRM_THREADS:-4}
    volumes:
      - osrm-data:/data
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:5000/route/v1/driving/-46.6333,-23.5505;-46.6334,-23.5506"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 300s
    restart: unless-stopped

  simulation:
    build:
      context: ../../services/simulation
      dockerfile: Dockerfile
      target: development
    container_name: rideshare-simulation
    deploy:
      resources:
        limits:
          memory: 1g
          cpus: "2.0"
        reservations:
          cpus: "0.25"
    profiles:
      - core
    ports:
      - "8000:8000"
    volumes:
      - ../../services/simulation/src:/app/src:ro
      - ../../services/simulation/data:/app/data:ro
      - simulation-db:/app/db
      - ./compose.yml:/app/compose.yml:ro
      - secrets-volume:/secrets:ro
    environment:
      - SIM_SPEED_MULTIPLIER=${SIM_SPEED_MULTIPLIER:-1}
      - SIM_LOG_LEVEL=${SIM_LOG_LEVEL:-INFO}
      - SIM_CHECKPOINT_INTERVAL=${SIM_CHECKPOINT_INTERVAL:-300}
      - SIM_CHECKPOINT_ENABLED=${SIM_CHECKPOINT_ENABLED:-true}
      - SIM_RESUME_FROM_CHECKPOINT=${SIM_RESUME_FROM_CHECKPOINT:-false}
      - SIM_DB_PATH=/app/db/simulation.db
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
      - KAFKA_SECURITY_PROTOCOL=SASL_PLAINTEXT
      - KAFKA_SCHEMA_REGISTRY_URL=http://schema-registry:8081
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=
      - REDIS_SSL=false
      - OSRM_BASE_URL=http://osrm:5000
      - CORS_ORIGINS=http://localhost:3000,http://localhost:5173
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
      - DEPLOYMENT_ENV=local
      - LOG_FORMAT=json
    entrypoint:
      - /bin/sh
      - -c
      - |
        set -a && eval "$$(grep '^KAFKA_SASL_' /secrets/core.env)" && set +a
        set -a && eval "$$(grep '^SCHEMA_REGISTRY_' /secrets/core.env)" && set +a
        set -a && eval "$$(grep '^REDIS_PASSWORD' /secrets/core.env)" && set +a
        set -a && eval "$$(grep '^API_KEY' /secrets/core.env)" && set +a
        export KAFKA_SCHEMA_REGISTRY_BASIC_AUTH_USER_INFO="$$SCHEMA_REGISTRY_USER:$$SCHEMA_REGISTRY_PASSWORD"
        exec python main.py
    depends_on:
      secrets-init:
        condition: service_completed_successfully
      kafka-init:
        condition: service_completed_successfully
      schema-registry:
        condition: service_healthy
      redis:
        condition: service_healthy
      osrm:
        condition: service_healthy
      stream-processor:
        condition: service_healthy
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 90s
    restart: unless-stopped

  stream-processor:
    build:
      context: ../../services/stream-processor
      dockerfile: Dockerfile
    container_name: rideshare-stream-processor
    deploy:
      resources:
        limits:
          memory: 256m
          cpus: "1.0"
        reservations:
          cpus: "0.25"
    profiles:
      - core
    ports:
      - "8080:8080"
    volumes:
      - secrets-volume:/secrets:ro
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
      - KAFKA_SECURITY_PROTOCOL=SASL_PLAINTEXT
      - KAFKA_GROUP_ID=stream-processor
      - KAFKA_AUTO_OFFSET_RESET=latest
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=
      - PROCESSOR_WINDOW_SIZE_MS=${PROCESSOR_WINDOW_SIZE_MS:-100}
      - PROCESSOR_AGGREGATION_STRATEGY=${PROCESSOR_AGGREGATION_STRATEGY:-latest}
      - PROCESSOR_TOPICS=gps_pings,trips,driver_status,surge_updates
      - API_HOST=0.0.0.0
      - API_PORT=8080
      - LOG_LEVEL=${PROCESSOR_LOG_LEVEL:-INFO}
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
      - DEPLOYMENT_ENV=local
      - LOG_FORMAT=json
    entrypoint:
      - /bin/sh
      - -c
      - |
        set -a && eval "$$(grep '^KAFKA_SASL_' /secrets/core.env)" && set +a
        set -a && eval "$$(grep '^REDIS_PASSWORD' /secrets/core.env)" && set +a
        exec python -m src.main
    depends_on:
      secrets-init:
        condition: service_completed_successfully
      kafka-init:
        condition: service_completed_successfully
      redis:
        condition: service_healthy
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  control-panel-frontend:
    build:
      context: ../../services/frontend
      dockerfile: Dockerfile
      target: development
    container_name: rideshare-frontend
    mem_limit: 256m
    profiles:
      - core
    ports:
      - "5173:5173"
    volumes:
      - ../../services/frontend:/app
      - frontend-node-modules:/app/node_modules
      - ../../services/simulation/data/zones.geojson:/app/public/zones.geojson:ro  # canonical source
    environment:
      - VITE_API_URL=http://localhost:8000
      - VITE_WS_URL=ws://localhost:8000/ws
    depends_on:
      simulation:
        condition: service_healthy
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "node", "-e", "require('http').get('http://localhost:5173', (r) => process.exit(r.statusCode === 200 ? 0 : 1)).on('error', () => process.exit(1))"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped

  minio:
    build:
      context: ../../services/minio
      dockerfile: Dockerfile
      args:
        MINIO_RELEASE: RELEASE.2025-10-15T17-29-55Z
    container_name: rideshare-minio
    mem_limit: 256m
    profiles:
      - data-pipeline
    ports:
      - "9000:9000"
      - "9001:9001"
    depends_on:
      secrets-init:
        condition: service_completed_successfully
    entrypoint:
      - /bin/sh
      - -c
      - |
        set -a && . /secrets/data-pipeline.env && set +a
        exec minio server /data --console-address ":9001"
    volumes:
      - minio-data:/data
      - secrets-volume:/secrets:ro
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  minio-init:
    image: quay.io/minio/mc:RELEASE.2025-08-13T08-35-41Z
    container_name: rideshare-minio-init
    profiles:
      - data-pipeline
    depends_on:
      secrets-init:
        condition: service_completed_successfully
      minio:
        condition: service_healthy
    volumes:
      - secrets-volume:/secrets:ro
    entrypoint: >
      /bin/sh -c "
      . /secrets/data-pipeline.env &&
      mc alias set local http://minio:9000 $$MINIO_ROOT_USER $$MINIO_ROOT_PASSWORD &&
      mc mb local/rideshare-bronze --ignore-existing &&
      mc mb local/rideshare-silver --ignore-existing &&
      mc mb local/rideshare-gold --ignore-existing &&
      mc mb local/rideshare-checkpoints --ignore-existing &&
      echo 'MinIO buckets initialized successfully'
      "
    networks:
      - rideshare-network

  bronze-ingestion:
    build:
      context: ../../services/bronze-ingestion
      dockerfile: Dockerfile
    container_name: rideshare-bronze-ingestion
    deploy:
      resources:
        limits:
          memory: 256m
          cpus: "0.5"
        reservations:
          cpus: "0.1"
    profiles:
      - data-pipeline
    ports:
      - "8086:8080"
    volumes:
      - secrets-volume:/secrets:ro
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
      - KAFKA_SECURITY_PROTOCOL=SASL_PLAINTEXT
      - KAFKA_CONSUMER_GROUP=bronze-ingestion
      - BATCH_INTERVAL_SECONDS=10
      - S3_ENDPOINT=http://minio:9000
      - BRONZE_BUCKET=rideshare-bronze
    entrypoint:
      - /bin/sh
      - -c
      - |
        set -a && eval "$$(grep '^KAFKA_SASL_' /secrets/core.env)" && set +a
        set -a && eval "$$(grep '^MINIO_' /secrets/data-pipeline.env)" && set +a
        export AWS_ACCESS_KEY_ID="$$MINIO_ROOT_USER"
        export AWS_SECRET_ACCESS_KEY="$$MINIO_ROOT_PASSWORD"
        exec python -m src.main
    depends_on:
      secrets-init:
        condition: service_completed_successfully
      minio-init:
        condition: service_completed_successfully
      kafka-init:
        condition: service_completed_successfully
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    restart: unless-stopped

  spark-thrift-server:
    image: apache/spark:4.0.0-python3
    container_name: rideshare-spark-thrift-server
    deploy:
      resources:
        limits:
          memory: 3072m
          cpus: "2.0"
        reservations:
          cpus: "0.25"
    profiles:
      - spark-testing
    ports:
      - "10000:10000"
      - "4041:4040"
    entrypoint:
      - /bin/sh
      - -c
      - |
        set -a && . /secrets/data-pipeline.env && set +a
        exec /opt/spark/bin/spark-submit \
          --master local[2] \
          --driver-memory 1800m \
          --conf spark.driver.memoryOverhead=512m \
          --conf spark.driver.maxResultSize=512m \
          --conf "spark.driver.extraJavaOptions=-XX:+UseG1GC -XX:MaxGCPauseMillis=50 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent" \
          --class org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 \
          --name "Thrift JDBC/ODBC Server" \
          --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \
          --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog \
          --conf spark.sql.warehouse.dir=s3a://rideshare-bronze/warehouse \
          --conf spark.hadoop.hive.metastore.uris=thrift://hive-metastore:9083 \
          --conf spark.ui.retainedStages=100 \
          --conf spark.ui.retainedJobs=100 \
          --conf spark.ui.retainedDeadExecutors=10 \
          --conf spark.sql.codegen.cache.maxEntries=100 \
          --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000 \
          --conf spark.hadoop.fs.s3a.access.key="$$MINIO_ROOT_USER" \
          --conf spark.hadoop.fs.s3a.secret.key="$$MINIO_ROOT_PASSWORD" \
          --conf spark.hadoop.fs.s3a.path.style.access=true \
          --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false \
          --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \
          --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider \
          --conf spark.hadoop.fs.s3a.threads.max=8 \
          --conf spark.hadoop.fs.s3a.connection.pool.size=8 \
          --conf spark.hadoop.fs.s3a.connection.establish.timeout=5000 \
          --conf spark.hadoop.fs.s3a.connection.timeout=200000 \
          --hiveconf hive.metastore.warehouse.dir=s3a://rideshare-bronze/warehouse \
          --hiveconf hive.server2.thrift.port=10000 \
          --hiveconf hive.server2.thrift.bind.host=0.0.0.0 \
          --hiveconf hive.server2.authentication=LDAP \
          --hiveconf hive.server2.authentication.ldap.url=ldap://openldap:389 \
          --hiveconf hive.server2.authentication.ldap.baseDN=dc=rideshare,dc=local \
          --hiveconf hive.server2.transport.mode=binary \
          --hiveconf hive.server2.thrift.min.worker.threads=4 \
          --hiveconf hive.server2.thrift.max.worker.threads=16
    environment:
      - PYTHONPATH=/opt
    volumes:
      - secrets-volume:/secrets:ro
    depends_on:
      openldap:
        condition: service_healthy
      hive-metastore:
        condition: service_healthy
      minio-init:
        condition: service_completed_successfully
      secrets-init:
        condition: service_completed_successfully
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD-SHELL", "/opt/spark/bin/beeline -u 'jdbc:hive2://localhost:10000/default' -n admin -p admin -e 'SELECT 1' --silent=true 2>/dev/null || exit 1"]
      interval: 15s
      timeout: 30s
      retries: 10
      start_period: 90s
    restart: unless-stopped

  localstack:
    image: localstack/localstack:4.12.0
    container_name: rideshare-localstack
    mem_limit: 256m
    profiles:
      - core
      - data-pipeline
      - monitoring
    ports:
      - "4566:4566"
      - "4510-4559:4510-4559"
    environment:
      - SERVICES=secretsmanager,sns,sqs
      - PERSISTENCE=0
      - LOCALSTACK_HOST=localhost:4566
      - DEBUG=0
      - LS_LOG=info
    volumes:
      - localstack-data:/var/lib/localstack
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4566/_localstack/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: unless-stopped

  secrets-init:
    image: python:3.12-slim
    container_name: rideshare-secrets-init
    profiles:
      - core
      - data-pipeline
      - monitoring
    volumes:
      - secrets-volume:/secrets
      - ../../infrastructure/scripts/seed-secrets.py:/app/seed-secrets.py:ro
      - ../../infrastructure/scripts/fetch-secrets.py:/app/fetch-secrets.py:ro
    environment:
      - AWS_ENDPOINT_URL=http://localstack:4566
      - AWS_ACCESS_KEY_ID=test
      - AWS_SECRET_ACCESS_KEY=test
      - AWS_DEFAULT_REGION=us-east-1
    depends_on:
      localstack:
        condition: service_healthy
    command: >
      sh -c "pip install boto3 mypy-boto3-secretsmanager &&
             python /app/seed-secrets.py &&
             python /app/fetch-secrets.py"
    networks:
      - rideshare-network
    restart: "no"

  postgres-airflow:
    image: postgres:16
    container_name: rideshare-postgres-airflow
    mem_limit: 256m
    profiles:
      - data-pipeline
    ports:
      - "5432:5432"
    depends_on:
      secrets-init:
        condition: service_completed_successfully
    environment:
      POSTGRES_DB: airflow
    entrypoint:
      - /bin/sh
      - -c
      - |
        set -a && . /secrets/data-pipeline.env && set +a
        export POSTGRES_USER="$$POSTGRES_AIRFLOW_USER"
        export POSTGRES_PASSWORD="$$POSTGRES_AIRFLOW_PASSWORD"
        exec docker-entrypoint.sh postgres
    volumes:
      - postgres-airflow-data:/var/lib/postgresql/data
      - secrets-volume:/secrets:ro
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD-SHELL", ". /secrets/data-pipeline.env && pg_isready -U \"$$POSTGRES_AIRFLOW_USER\""]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped

  airflow-webserver:
    image: apache/airflow:3.1.5
    container_name: rideshare-airflow-webserver
    deploy:
      resources:
        limits:
          memory: 384m
          cpus: "1.0"
        reservations:
          cpus: "0.1"
    profiles:
      - data-pipeline
    ports:
      - "8082:8080"
    environment:
      PROD_MODE: ${PROD_MODE:-false}
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__CORE__AUTH_MANAGER: 'airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager'
      AIRFLOW__CORE__PARALLELISM: 8
      AIRFLOW__DAG_PROCESSOR__REFRESH_INTERVAL: 30
      AIRFLOW__CORE__EXECUTION_API_SERVER_URL: 'http://airflow-webserver:8080/execution/'
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _PIP_ADDITIONAL_REQUIREMENTS: 'apache-airflow-providers-fab dbt-core dbt-duckdb==1.10.0 dbt-spark[PyHive] duckdb==1.4.4 deltalake==1.4.2 duckdb-engine==0.17.0 great-expectations requests prison'
    volumes:
      - ../../services/airflow/dags:/opt/airflow/dags
      - ../../services/airflow/logs:/opt/airflow/logs
      - ../../services/airflow/plugins:/opt/airflow/plugins
      - ../../services/airflow/config:/opt/airflow/config
      - ../../tools/dbt:/opt/dbt
      - ../../tools/great-expectations:/opt/great-expectations
      - secrets-volume:/secrets:ro
    depends_on:
      secrets-init:
        condition: service_completed_successfully
      postgres-airflow:
        condition: service_healthy
    networks:
      - rideshare-network
    entrypoint:
      - /bin/sh
      - -c
      - |
        set -a && . /secrets/data-pipeline.env && set +a
        export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="postgresql+psycopg2://$$POSTGRES_AIRFLOW_USER:$$POSTGRES_AIRFLOW_PASSWORD@postgres-airflow:5432/airflow"
        export _AIRFLOW_WWW_USER_USERNAME="$$AIRFLOW_ADMIN_USERNAME"
        export _AIRFLOW_WWW_USER_PASSWORD="$$AIRFLOW_ADMIN_PASSWORD"
        exec /usr/bin/dumb-init -- /entrypoint airflow api-server
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8080/api/v2/monitor/health', timeout=3)"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 600s
    restart: unless-stopped

  airflow-scheduler:
    image: apache/airflow:3.1.5
    container_name: rideshare-airflow-scheduler
    deploy:
      resources:
        limits:
          memory: 768m
          cpus: "1.5"
        reservations:
          cpus: "0.25"
    profiles:
      - data-pipeline
    environment:
      PROD_MODE: ${PROD_MODE:-false}
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__CORE__AUTH_MANAGER: 'airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager'
      AIRFLOW__CORE__PARALLELISM: 8
      AIRFLOW__DAG_PROCESSOR__REFRESH_INTERVAL: 30
      AIRFLOW__CORE__EXECUTION_API_SERVER_URL: 'http://airflow-webserver:8080/execution/'
      DBT_SPARK_HOST: spark-thrift-server
      # S3/MinIO non-secret config (credentials sourced at runtime from secrets volume)
      AWS_ENDPOINT_URL: http://minio:9000
      AWS_ALLOW_HTTP: "true"
      AWS_REGION: us-east-1
      _PIP_ADDITIONAL_REQUIREMENTS: 'apache-airflow-providers-fab dbt-core dbt-duckdb==1.10.0 dbt-spark[PyHive] duckdb==1.4.4 deltalake==1.4.2 duckdb-engine==0.17.0 great-expectations requests prison'
    volumes:
      - ../../services/airflow/dags:/opt/airflow/dags
      - ../../services/airflow/logs:/opt/airflow/logs
      - ../../services/airflow/plugins:/opt/airflow/plugins
      - ../../services/airflow/config:/opt/airflow/config
      - ../../tools/dbt:/opt/dbt
      - ../../tools/great-expectations:/opt/great-expectations
      - ../../infrastructure/scripts:/opt/init-scripts:ro
      - secrets-volume:/secrets:ro
    depends_on:
      secrets-init:
        condition: service_completed_successfully
      postgres-airflow:
        condition: service_healthy
      airflow-webserver:
        condition: service_healthy  # Ensure DB migrations complete before reserialization
      spark-thrift-server:
        condition: service_healthy
        required: false
    networks:
      - rideshare-network
    entrypoint:
      - /bin/sh
      - -c
      - |
        set -a && . /secrets/data-pipeline.env && set +a
        export AIRFLOW__DATABASE__SQL_ALCHEMY_CONN="postgresql+psycopg2://$$POSTGRES_AIRFLOW_USER:$$POSTGRES_AIRFLOW_PASSWORD@postgres-airflow:5432/airflow"
        export AWS_ACCESS_KEY_ID="$$MINIO_ROOT_USER"
        export AWS_SECRET_ACCESS_KEY="$$MINIO_ROOT_PASSWORD"
        airflow dags reserialize && exec /usr/bin/dumb-init -- /entrypoint airflow scheduler
    healthcheck:
      test: ["CMD", "python", "-c", "import socket; s = socket.socket(); s.settimeout(3); s.connect(('localhost', 8793)); s.close()"]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped

  postgres-metastore:
    image: postgres:16
    container_name: rideshare-postgres-metastore
    mem_limit: 256m
    profiles:
      - data-pipeline
    ports:
      - "5434:5432"
    depends_on:
      secrets-init:
        condition: service_completed_successfully
    environment:
      POSTGRES_DB: metastore
    entrypoint:
      - /bin/sh
      - -c
      - |
        set -a && . /secrets/data-pipeline.env && set +a
        export POSTGRES_USER="$$POSTGRES_METASTORE_USER"
        export POSTGRES_PASSWORD="$$POSTGRES_METASTORE_PASSWORD"
        exec docker-entrypoint.sh postgres
    volumes:
      - postgres-metastore-data:/var/lib/postgresql/data
      - secrets-volume:/secrets:ro
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD-SHELL", ". /secrets/data-pipeline.env && pg_isready -U \"$$POSTGRES_METASTORE_USER\""]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped

  hive-metastore:
    build:
      context: ../../services/hive-metastore
      dockerfile: Dockerfile
    container_name: rideshare-hive-metastore
    mem_limit: 1152m
    profiles:
      - data-pipeline
    ports:
      - "9083:9083"
    environment:
      SERVICE_NAME: metastore
      DB_DRIVER: postgres
      SERVICE_OPTS: >-
        -Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver
        -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres-metastore:5432/metastore
    depends_on:
      secrets-init:
        condition: service_completed_successfully
      postgres-metastore:
        condition: service_healthy
      minio:
        condition: service_healthy
    volumes:
      - ../../services/hive-metastore/hive-site.xml.template:/opt/hive/conf/hive-site.xml.template:ro
      - secrets-volume:/secrets:ro
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD-SHELL", "bash -c 'echo > /dev/tcp/localhost/9083' || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 120s
    restart: unless-stopped

  openldap:
    image: osixia/openldap:1.5.0
    container_name: rideshare-openldap
    mem_limit: 256m
    profiles:
      - data-pipeline
    ports:
      - "389:389"
    environment:
      LDAP_ORGANISATION: "Rideshare Platform"
      LDAP_DOMAIN: "rideshare.local"
      LDAP_TLS: "false"
    entrypoint:
      - /bin/sh
      - -c
      - |
        set -a && . /secrets/data-pipeline.env && set +a
        exec /container/tool/run --copy-service
    depends_on:
      secrets-init:
        condition: service_completed_successfully
    volumes:
      - openldap-data:/var/lib/ldap
      - openldap-config:/etc/ldap/slapd.d
      - secrets-volume:/secrets:ro
      - ../../infrastructure/openldap/bootstrap.ldif:/container/service/slapd/assets/config/bootstrap/ldif/custom/bootstrap.ldif:ro
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD-SHELL", ". /secrets/data-pipeline.env && ldapsearch -x -H ldap://localhost:389 -b dc=rideshare,dc=local -D cn=admin,dc=rideshare,dc=local -w \"$$LDAP_ADMIN_PASSWORD\" -s base"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: unless-stopped

  trino:
    image: trinodb/trino:439
    container_name: rideshare-trino
    entrypoint: ["/custom-entrypoint.sh"]
    deploy:
      resources:
        limits:
          memory: 2g
          cpus: "2.0"
        reservations:
          cpus: "0.25"
    profiles:
      - data-pipeline
    ports:
      - "8084:8080"
    environment:
      - TRINO_ENVIRONMENT=docker
    volumes:
      - ../../services/trino/etc:/etc/trino
      - ../../services/trino/entrypoint.sh:/custom-entrypoint.sh:ro
      - secrets-volume:/secrets:ro
    depends_on:
      secrets-init:
        condition: service_completed_successfully
      hive-metastore:
        condition: service_healthy
      minio:
        condition: service_healthy
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/v1/info"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 60s
    restart: unless-stopped

  delta-table-init:
    image: trinodb/trino:439
    container_name: rideshare-delta-table-init
    profiles:
      - data-pipeline
    depends_on:
      trino:
        condition: service_healthy
      minio-init:
        condition: service_completed_successfully
    environment:
      - TRINO_HOST=trino
      - TRINO_PORT=8080
    volumes:
      - ../../infrastructure/scripts:/opt/init-scripts:ro
    entrypoint: ["/bin/bash", "/opt/init-scripts/register-delta-tables.sh"]
    networks:
      - rideshare-network

  prometheus:
    image: prom/prometheus:v3.9.1
    container_name: rideshare-prometheus
    mem_limit: 512m
    profiles:
      - monitoring
    ports:
      - "9090:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=7d'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
      - '--web.enable-remote-write-receiver'
    volumes:
      - ../../services/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ../../services/prometheus/rules:/etc/prometheus/rules:ro
      - prometheus-data:/prometheus
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  cadvisor:
    image: ghcr.io/google/cadvisor:v0.53.0
    container_name: rideshare-cadvisor
    mem_limit: 256m
    profiles:
      - monitoring
    ports:
      - "8083:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    devices:
      - /dev/kmsg
    networks:
      - rideshare-network
    privileged: true
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8080/healthz"]
      interval: 10s
      timeout: 5s
      retries: 5

  grafana:
    image: grafana/grafana:12.3.1
    container_name: rideshare-grafana
    mem_limit: 384m
    profiles:
      - monitoring
    ports:
      - "3001:3000"
    environment:
      GF_PATHS_PROVISIONING: /etc/grafana/provisioning
      GF_SERVER_ROOT_URL: http://localhost:3001
      GF_AUTH_ANONYMOUS_ENABLED: "false"
      GF_INSTALL_PLUGINS: trino-datasource
    entrypoint:
      - /bin/sh
      - -c
      - |
        set -a && . /secrets/monitoring.env && set +a
        exec /run.sh
    volumes:
      - ../../services/grafana/provisioning/datasources:/etc/grafana/provisioning/datasources:ro
      - ../../services/grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ../../services/grafana/dashboards:/etc/dashboards:ro
      - ../../services/grafana/provisioning/alerting:/etc/grafana/provisioning/alerting:ro
      - grafana-data:/var/lib/grafana
      - secrets-volume:/secrets:ro
    depends_on:
      secrets-init:
        condition: service_completed_successfully
      prometheus:
        condition: service_healthy
      loki:
        condition: service_healthy
      tempo:
        condition: service_healthy
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped

  loki:
    image: grafana/loki:3.6.5
    container_name: rideshare-loki
    mem_limit: 384m
    profiles:
      - monitoring
    ports:
      - "3100:3100"
    volumes:
      - ../../services/loki/loki-config.yaml:/etc/loki/local-config.yaml:ro
      - loki-data:/loki
    command: -config.file=/etc/loki/local-config.yaml
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "/usr/bin/loki", "-health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped

  tempo:
    build:
      context: ../../services/tempo
      dockerfile: Dockerfile
    container_name: rideshare-tempo
    mem_limit: 768m
    profiles:
      - monitoring
    ports:
      - "3200:3200"   # Tempo HTTP (query API)
      - "4319:4317"   # OTLP gRPC (from OTel Collector)
    volumes:
      - ../../services/tempo/tempo-config.yaml:/etc/tempo/tempo.yaml:ro
      - tempo-data:/var/tempo
    command: ["-config.file=/etc/tempo/tempo.yaml"]
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3200/ready"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped

  otel-collector:
    build:
      context: ../../services/otel-collector
      dockerfile: Dockerfile
    container_name: rideshare-otel-collector
    user: "0:0"
    mem_limit: 256m
    profiles:
      - monitoring
    ports:
      - "4317:4317"   # OTLP gRPC receiver
      - "4318:4318"   # OTLP HTTP receiver
      - "8888:8888"   # Collector metrics
    volumes:
      - ../../services/otel-collector/otel-collector-config.yaml:/etc/otelcol-contrib/config.yaml:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    command: ["--config=/etc/otelcol-contrib/config.yaml"]
    depends_on:
      prometheus:
        condition: service_healthy
      loki:
        condition: service_healthy
      tempo:
        condition: service_healthy
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:13133/"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped

networks:
  rideshare-network:
    driver: bridge
    name: rideshare-network

volumes:
  kafka-data:
  redis-data:
  osrm-data:
  simulation-db:
  frontend-node-modules:
  minio-data:
  localstack-data:
  postgres-airflow-data:
  postgres-metastore-data:
  prometheus-data:
  grafana-data:
  loki-data:
  tempo-data:
  openldap-data:
  openldap-config:
  secrets-volume:
