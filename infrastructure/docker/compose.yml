name: rideshare-platform

# Docker Compose Profiles:
# - core: Simulation runtime services (kafka, redis, osrm, simulation, stream-processor, frontend)
# - data-pipeline: ETL, streaming, and orchestration (minio, spark-*, localstack, airflow, hive-metastore, trino)
# - monitoring: Observability (prometheus, cadvisor, grafana, otel-collector, loki, tempo)

services:
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: rideshare-kafka
    deploy:
      resources:
        limits:
          memory: 1g
          cpus: "1.5"
        reservations:
          cpus: "0.25"
    profiles:
      - core
      - data-pipeline
    ports:
      - "9092:9092"
    environment:
      # JVM flags for ARM/Apple Silicon stability
      KAFKA_OPTS: "-XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent"
      KAFKA_HEAP_OPTS: "-Xms256m -Xmx512m"

      # Log retention - auto-delete since no consumers yet
      KAFKA_LOG_RETENTION_HOURS: 1
      KAFKA_LOG_RETENTION_BYTES: 536870912  # 512MB per partition

      # Threading - increased for 23 partitions + 3 consumer groups
      KAFKA_NUM_NETWORK_THREADS: "6"
      KAFKA_NUM_IO_THREADS: "16"
      # Socket buffers
      KAFKA_SOCKET_SEND_BUFFER_BYTES: "102400"
      KAFKA_SOCKET_RECEIVE_BUFFER_BYTES: "102400"

      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_LISTENERS: PLAINTEXT://kafka:29092,CONTROLLER://kafka:29093,PLAINTEXT_HOST://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:29093
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_LOG_DIRS: /tmp/kraft-combined-logs
      CLUSTER_ID: MkU3OEVBNTcwNTJENDM2Qk
    volumes:
      - kafka-data:/var/lib/kafka/data
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 10s
      timeout: 10s
      retries: 10
    restart: unless-stopped

  kafka-init:
    image: confluentinc/cp-kafka:7.5.0
    container_name: rideshare-kafka-init
    profiles:
      - core
      - data-pipeline
    depends_on:
      kafka:
        condition: service_healthy
    volumes:
      - ../../services/kafka/topics.yaml:/etc/kafka/topics.yaml:ro
      - ../../services/kafka/create-topics.sh:/etc/kafka/create-topics.sh:ro
    entrypoint: ["/bin/sh", "/etc/kafka/create-topics.sh"]
    networks:
      - rideshare-network

  schema-registry:
    image: confluentinc/cp-schema-registry:7.5.0
    container_name: rideshare-schema-registry
    deploy:
      resources:
        limits:
          memory: 512m
          cpus: "0.5"
        reservations:
          cpus: "0.1"
    profiles:
      - core
      - data-pipeline
    ports:
      - "8085:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka:29092
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
      SCHEMA_REGISTRY_HEAP_OPTS: "-Xms128m -Xmx256m"
    depends_on:
      kafka-init:
        condition: service_completed_successfully
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/subjects"]
      interval: 10s
      timeout: 10s
      retries: 12
      start_period: 30s
    restart: unless-stopped

  redis:
    image: redis:8.0-alpine
    container_name: rideshare-redis
    mem_limit: 128m
    profiles:
      - core
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5
    restart: unless-stopped

  osrm:
    build:
      context: ../../services/osrm
      dockerfile: Dockerfile
      args:
        OSRM_MAP_SOURCE: ${OSRM_MAP_SOURCE:-local}
    container_name: rideshare-osrm
    mem_limit: 1g
    profiles:
      - core
    platform: linux/amd64
    ports:
      - "5050:5000"
    environment:
      - OSRM_THREADS=${OSRM_THREADS:-4}
    volumes:
      - osrm-data:/data
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:5000/route/v1/driving/-46.6333,-23.5505;-46.6334,-23.5506"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 180s
    restart: unless-stopped

  simulation:
    build:
      context: ../../services/simulation
      dockerfile: Dockerfile
      target: development
    container_name: rideshare-simulation
    deploy:
      resources:
        limits:
          memory: 1g
          cpus: "2.0"
        reservations:
          cpus: "0.25"
    profiles:
      - core
    ports:
      - "8000:8000"
    volumes:
      - ../../services/simulation/src:/app/src:ro
      - ../../services/simulation/data:/app/data:ro
      - simulation-db:/app/db
      - ./compose.yml:/app/compose.yml:ro
    environment:
      - SIM_SPEED_MULTIPLIER=${SIM_SPEED_MULTIPLIER:-1}
      - SIM_LOG_LEVEL=${SIM_LOG_LEVEL:-INFO}
      - SIM_CHECKPOINT_INTERVAL=${SIM_CHECKPOINT_INTERVAL:-300}
      - SIM_DB_PATH=/app/db/simulation.db
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
      - KAFKA_SECURITY_PROTOCOL=PLAINTEXT
      - KAFKA_SCHEMA_REGISTRY_URL=http://schema-registry:8081
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=
      - REDIS_SSL=false
      - OSRM_BASE_URL=http://osrm:5000
      - API_KEY=${API_KEY:-dev-api-key-change-in-production}
      - CORS_ORIGINS=http://localhost:3000,http://localhost:5173
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
      - DEPLOYMENT_ENV=local
      - LOG_FORMAT=json
    depends_on:
      kafka-init:
        condition: service_completed_successfully
      schema-registry:
        condition: service_healthy
      redis:
        condition: service_healthy
      osrm:
        condition: service_healthy
      stream-processor:
        condition: service_healthy
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 90s
    restart: unless-stopped

  stream-processor:
    build:
      context: ../../services/stream-processor
      dockerfile: Dockerfile
    container_name: rideshare-stream-processor
    deploy:
      resources:
        limits:
          memory: 256m
          cpus: "1.0"
        reservations:
          cpus: "0.25"
    profiles:
      - core
    ports:
      - "8080:8080"
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
      - KAFKA_GROUP_ID=stream-processor
      - KAFKA_AUTO_OFFSET_RESET=latest
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=
      - PROCESSOR_WINDOW_SIZE_MS=${PROCESSOR_WINDOW_SIZE_MS:-100}
      - PROCESSOR_AGGREGATION_STRATEGY=${PROCESSOR_AGGREGATION_STRATEGY:-latest}
      - PROCESSOR_TOPICS=gps_pings,trips,driver_status,surge_updates
      - API_HOST=0.0.0.0
      - API_PORT=8080
      - LOG_LEVEL=${PROCESSOR_LOG_LEVEL:-INFO}
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
      - DEPLOYMENT_ENV=local
      - LOG_FORMAT=json
    depends_on:
      kafka-init:
        condition: service_completed_successfully
      redis:
        condition: service_healthy
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  control-panel-frontend:
    build:
      context: ../../services/frontend
      dockerfile: Dockerfile
      target: development
    container_name: rideshare-frontend
    mem_limit: 384m
    profiles:
      - core
    ports:
      - "5173:5173"
    volumes:
      - ../../services/frontend:/app
      - frontend-node-modules:/app/node_modules
      - ../../services/simulation/data/zones.geojson:/app/public/zones.geojson:ro  # canonical source
    environment:
      - VITE_API_URL=http://localhost:8000
      - VITE_WS_URL=ws://localhost:8000/ws
    depends_on:
      simulation:
        condition: service_healthy
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "node", "-e", "require('http').get('http://localhost:5173', (r) => process.exit(r.statusCode === 200 ? 0 : 1)).on('error', () => process.exit(1))"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped

  minio:
    build:
      context: ../../services/minio
      dockerfile: Dockerfile
      args:
        MINIO_RELEASE: RELEASE.2025-10-15T17-29-55Z
    container_name: rideshare-minio
    mem_limit: 256m
    profiles:
      - data-pipeline
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001"
    volumes:
      - minio-data:/data
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  minio-init:
    image: quay.io/minio/mc:RELEASE.2025-08-13T08-35-41Z
    container_name: rideshare-minio-init
    profiles:
      - data-pipeline
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      mc alias set local http://minio:9000 minioadmin minioadmin &&
      mc mb local/rideshare-bronze --ignore-existing &&
      mc mb local/rideshare-silver --ignore-existing &&
      mc mb local/rideshare-gold --ignore-existing &&
      mc mb local/rideshare-checkpoints --ignore-existing &&
      echo 'MinIO buckets initialized successfully'
      "
    networks:
      - rideshare-network

  # ============================================================================
  # COMMENTED OUT: Spark Cluster Mode (migrated to local mode 2026-01-19)
  # Each Spark service now runs in local mode within its container.
  # Preserved for reference - uncomment to restore cluster mode if needed.
  # ============================================================================
  # spark-master:
  #   build:
  #     context: ../..
  #     dockerfile: infrastructure/docker/dockerfiles/spark-delta.Dockerfile
  #   container_name: rideshare-spark-master
  #   mem_limit: 512m
  #   profiles:
  #     - data-platform
  #   ports:
  #     - "7077:7077"
  #     - "4040:8080"
  #   command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
  #   environment:
  #     - PYTHONPATH=/opt
  #   volumes:
  #     - ../../services/spark-streaming:/opt/spark_streaming:ro
  #   networks:
  #     - rideshare-network
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:8080"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 10
  #     start_period: 30s
  #
  # spark-worker:
  #   build:
  #     context: ../..
  #     dockerfile: infrastructure/docker/dockerfiles/spark-delta.Dockerfile
  #   container_name: rideshare-spark-worker
  #   mem_limit: 3g
  #   profiles:
  #     - data-platform
  #   ports:
  #     - "8081:8081"
  #   command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker -m 2g -c 4 --webui-port 8081 spark://spark-master:7077
  #   environment:
  #     - PYTHONPATH=/opt
  #   volumes:
  #     - ../../services/spark-streaming:/opt/spark_streaming:ro
  #   depends_on:
  #     spark-master:
  #       condition: service_healthy
  #     minio:
  #       condition: service_healthy
  #   networks:
  #     - rideshare-network
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:8081"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 10
  #     start_period: 30s

  spark-thrift-server:
    build:
      context: ../..
      dockerfile: services/spark-streaming/Dockerfile
    container_name: rideshare-spark-thrift-server
    deploy:
      resources:
        limits:
          memory: 3072m
          cpus: "2.0"
        reservations:
          cpus: "0.25"
    profiles:
      - data-pipeline
    ports:
      - "10000:10000"
      - "4041:4040"
    command: >
      /opt/spark/bin/spark-submit
      --master local[2]
      --driver-memory 1800m
      --conf spark.driver.memoryOverhead=512m
      --conf spark.driver.maxResultSize=512m
      --conf "spark.driver.extraJavaOptions=-XX:+UseG1GC -XX:MaxGCPauseMillis=50 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent"
      --class org.apache.spark.sql.hive.thriftserver.HiveThriftServer2
      --name "Thrift JDBC/ODBC Server"
      --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
      --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog
      --conf spark.sql.warehouse.dir=s3a://rideshare-bronze/warehouse
      --conf spark.hadoop.hive.metastore.uris=thrift://hive-metastore:9083
      --conf spark.ui.retainedStages=100
      --conf spark.ui.retainedJobs=100
      --conf spark.ui.retainedDeadExecutors=10
      --conf spark.sql.codegen.cache.maxEntries=100
      --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000
      --conf spark.hadoop.fs.s3a.access.key=minioadmin
      --conf spark.hadoop.fs.s3a.secret.key=minioadmin
      --conf spark.hadoop.fs.s3a.path.style.access=true
      --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false
      --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
      --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
      --conf spark.hadoop.fs.s3a.threads.max=8
      --conf spark.hadoop.fs.s3a.connection.pool.size=8
      --conf spark.hadoop.fs.s3a.connection.establish.timeout=5000
      --conf spark.hadoop.fs.s3a.connection.timeout=200000
      --hiveconf hive.metastore.warehouse.dir=s3a://rideshare-bronze/warehouse
      --hiveconf hive.server2.thrift.port=10000
      --hiveconf hive.server2.thrift.bind.host=0.0.0.0
      --hiveconf hive.server2.authentication=NOSASL
      --hiveconf hive.server2.transport.mode=binary
      --hiveconf hive.server2.thrift.min.worker.threads=4
      --hiveconf hive.server2.thrift.max.worker.threads=16
    environment:
      - PYTHONPATH=/opt
    volumes:
      - ../../services/spark-streaming:/opt/spark_streaming:ro
    depends_on:
      hive-metastore:
        condition: service_healthy
      minio-init:
        condition: service_completed_successfully
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD-SHELL", "/opt/spark/bin/beeline -u 'jdbc:hive2://localhost:10000/default;auth=noSasl' -n healthcheck -e 'SELECT 1' --silent=true 2>/dev/null || exit 1"]
      interval: 15s
      timeout: 30s
      retries: 10
      start_period: 90s
    restart: unless-stopped

  bronze-ingestion-high-volume:
    build:
      context: ../..
      dockerfile: services/spark-streaming/Dockerfile
    container_name: rideshare-bronze-ingestion-high-volume
    deploy:
      resources:
        limits:
          memory: 2g
          cpus: "2.0"
        reservations:
          cpus: "0.25"
    profiles:
      - data-pipeline
    environment:
      - PYTHONPATH=/opt
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
      - SCHEMA_REGISTRY_URL=http://schema-registry:8081
      - KAFKA_TOPICS=gps_pings
      - CHECKPOINT_BASE_PATH=s3a://rideshare-checkpoints/
      - TRIGGER_INTERVAL=10 seconds
    volumes:
      - ../../services/spark-streaming:/opt/spark_streaming:ro
    depends_on:
      minio-init:
        condition: service_completed_successfully
      kafka-init:
        condition: service_completed_successfully
    networks:
      - rideshare-network
    command: >
      /opt/spark/bin/spark-submit
      --master local[2]
      --driver-memory 1536m
      --conf "spark.driver.extraJavaOptions=-XX:+UseG1GC -XX:MaxGCPauseMillis=50 -XX:InitiatingHeapOccupancyPercent=35"
      --name "streaming_high_volume"
      --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
      --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog
      --conf spark.driver.memoryOverhead=256m
      --conf spark.driver.maxResultSize=256m
      --conf spark.ui.retainedStages=100
      --conf spark.ui.retainedJobs=100
      --conf spark.ui.retainedDeadExecutors=10
      --conf spark.sql.codegen.cache.maxEntries=100
      --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000
      --conf spark.hadoop.fs.s3a.access.key=minioadmin
      --conf spark.hadoop.fs.s3a.secret.key=minioadmin
      --conf spark.hadoop.fs.s3a.path.style.access=true
      --conf spark.hadoop.fs.s3a.threads.max=8
      --conf spark.hadoop.fs.s3a.connection.pool.size=8
      /opt/spark_streaming/jobs/bronze_ingestion_high_volume.py
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f 'spark-submit' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  bronze-ingestion-low-volume:
    build:
      context: ../..
      dockerfile: services/spark-streaming/Dockerfile
    container_name: rideshare-bronze-ingestion-low-volume
    deploy:
      resources:
        limits:
          memory: 2g
          cpus: "2.0"
        reservations:
          cpus: "0.25"
    profiles:
      - data-pipeline
    environment:
      - PYTHONPATH=/opt
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
      - SCHEMA_REGISTRY_URL=http://schema-registry:8081
      - KAFKA_TOPICS=trips,driver_status,surge_updates,ratings,payments,driver_profiles,rider_profiles
      - CHECKPOINT_BASE_PATH=s3a://rideshare-checkpoints/
      - TRIGGER_INTERVAL=30 seconds
    volumes:
      - ../../services/spark-streaming:/opt/spark_streaming:ro
    depends_on:
      minio-init:
        condition: service_completed_successfully
      kafka-init:
        condition: service_completed_successfully
    networks:
      - rideshare-network
    command: >
      /opt/spark/bin/spark-submit
      --master local[2]
      --driver-memory 1536m
      --conf "spark.driver.extraJavaOptions=-XX:+UseG1GC -XX:MaxGCPauseMillis=50 -XX:InitiatingHeapOccupancyPercent=35"
      --name "streaming_low_volume"
      --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
      --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog
      --conf spark.driver.memoryOverhead=256m
      --conf spark.driver.maxResultSize=256m
      --conf spark.ui.retainedStages=100
      --conf spark.ui.retainedJobs=100
      --conf spark.ui.retainedDeadExecutors=10
      --conf spark.sql.codegen.cache.maxEntries=100
      --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000
      --conf spark.hadoop.fs.s3a.access.key=minioadmin
      --conf spark.hadoop.fs.s3a.secret.key=minioadmin
      --conf spark.hadoop.fs.s3a.path.style.access=true
      --conf spark.hadoop.fs.s3a.threads.max=8
      --conf spark.hadoop.fs.s3a.connection.pool.size=8
      /opt/spark_streaming/jobs/bronze_ingestion_low_volume.py
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f 'spark-submit' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  bronze-init:
    image: apache/airflow:3.1.5
    container_name: rideshare-bronze-init
    profiles:
      - data-pipeline
    environment:
      - _PIP_ADDITIONAL_REQUIREMENTS=pyhive thrift
    volumes:
      - ../../infrastructure/scripts:/opt/init-scripts:ro
    depends_on:
      spark-thrift-server:
        condition: service_healthy
    networks:
      - rideshare-network
    command: bash /opt/init-scripts/wait-for-thrift-and-init-bronze.sh
    restart: "no"

  localstack:
    image: localstack/localstack:4.12.0
    container_name: rideshare-localstack
    mem_limit: 384m
    profiles:
      - data-pipeline
    ports:
      - "4566:4566"
      - "4510-4559:4510-4559"
    environment:
      - SERVICES=secretsmanager,sns,sqs
      - PERSISTENCE=0
      - LOCALSTACK_HOST=localhost:4566
      - DEBUG=0
      - LS_LOG=info
    volumes:
      - localstack-data:/var/lib/localstack
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4566/_localstack/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: unless-stopped

  postgres-airflow:
    image: postgres:16
    container_name: rideshare-postgres-airflow
    mem_limit: 256m
    profiles:
      - data-pipeline
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-airflow-data:/var/lib/postgresql/data
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped

  airflow-webserver:
    image: apache/airflow:3.1.5
    container_name: rideshare-airflow-webserver
    deploy:
      resources:
        limits:
          memory: 384m
          cpus: "1.0"
        reservations:
          cpus: "0.1"
    profiles:
      - data-pipeline
    ports:
      - "8082:8080"
    environment:
      PROD_MODE: ${PROD_MODE:-false}
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres-airflow/airflow
      AIRFLOW__CORE__FERNET_KEY: "989ZwyjTUPqrIF5wxv6t2co6yYH94e5BVlrefZQ-Vy8="
      AIRFLOW__CORE__INTERNAL_API_SECRET_KEY: "-mszrjJhmspCQdvqihrR6DIrme0unbToVPUk40brcPE="
      AIRFLOW__API_AUTH__JWT_SECRET: "LZzI1GGTMbhAuzQhC9wfN7n0yaVe3ANAUsTl6jVbVSw"
      AIRFLOW__API__SECRET_KEY: "D2rCEm60_xUtsSSxMXh9tOEWGgEadUX39FRaMesKvl0"
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__CORE__AUTH_MANAGER: 'airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager'
      AIRFLOW__CORE__PARALLELISM: 8
      AIRFLOW__DAG_PROCESSOR__REFRESH_INTERVAL: 30
      AIRFLOW__CORE__EXECUTION_API_SERVER_URL: 'http://airflow-webserver:8080/execution/'
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: admin
      _AIRFLOW_WWW_USER_PASSWORD: admin
      _PIP_ADDITIONAL_REQUIREMENTS: 'apache-airflow-providers-fab apache-airflow-providers-apache-spark pyhive thrift dbt-core dbt-spark[PyHive] great-expectations requests prison'
    volumes:
      - ../../services/airflow/dags:/opt/airflow/dags
      - ../../services/airflow/logs:/opt/airflow/logs
      - ../../services/airflow/plugins:/opt/airflow/plugins
      - ../../services/airflow/config:/opt/airflow/config
      - ../../tools/dbt:/opt/dbt
      - ../../tools/great-expectations:/opt/great-expectations
    depends_on:
      postgres-airflow:
        condition: service_healthy
    networks:
      - rideshare-network
    command: api-server
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8080/api/v2/monitor/health', timeout=3)"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 600s
    restart: unless-stopped

  airflow-scheduler:
    image: apache/airflow:3.1.5
    container_name: rideshare-airflow-scheduler
    deploy:
      resources:
        limits:
          memory: 768m
          cpus: "1.5"
        reservations:
          cpus: "0.25"
    profiles:
      - data-pipeline
    environment:
      PROD_MODE: ${PROD_MODE:-false}
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres-airflow/airflow
      AIRFLOW__CORE__FERNET_KEY: "989ZwyjTUPqrIF5wxv6t2co6yYH94e5BVlrefZQ-Vy8="
      AIRFLOW__CORE__INTERNAL_API_SECRET_KEY: "-mszrjJhmspCQdvqihrR6DIrme0unbToVPUk40brcPE="
      AIRFLOW__API_AUTH__JWT_SECRET: "LZzI1GGTMbhAuzQhC9wfN7n0yaVe3ANAUsTl6jVbVSw"
      AIRFLOW__API__SECRET_KEY: "D2rCEm60_xUtsSSxMXh9tOEWGgEadUX39FRaMesKvl0"
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__CORE__AUTH_MANAGER: 'airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager'
      AIRFLOW__CORE__PARALLELISM: 8
      AIRFLOW__DAG_PROCESSOR__REFRESH_INTERVAL: 30
      AIRFLOW__CORE__EXECUTION_API_SERVER_URL: 'http://airflow-webserver:8080/execution/'
      DBT_SPARK_HOST: spark-thrift-server
      _PIP_ADDITIONAL_REQUIREMENTS: 'apache-airflow-providers-fab apache-airflow-providers-apache-spark pyhive thrift dbt-core dbt-spark[PyHive] great-expectations requests prison'
    volumes:
      - ../../services/airflow/dags:/opt/airflow/dags
      - ../../services/airflow/logs:/opt/airflow/logs
      - ../../services/airflow/plugins:/opt/airflow/plugins
      - ../../services/airflow/config:/opt/airflow/config
      - ../../tools/dbt:/opt/dbt
      - ../../tools/great-expectations:/opt/great-expectations
      - ../../infrastructure/scripts:/opt/init-scripts:ro
    depends_on:
      postgres-airflow:
        condition: service_healthy
      airflow-webserver:
        condition: service_healthy  # Ensure DB migrations complete before reserialization
      spark-thrift-server:
        condition: service_healthy  # Required for DBT tasks to connect
    networks:
      - rideshare-network
    # Reserialize DAGs on startup to ensure they're loaded in Airflow 3.x
    command: bash -c "airflow dags reserialize && exec airflow scheduler"
    healthcheck:
      test: ["CMD", "python", "-c", "import socket; s = socket.socket(); s.settimeout(3); s.connect(('localhost', 8793)); s.close()"]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped

  postgres-metastore:
    image: postgres:16
    container_name: rideshare-postgres-metastore
    mem_limit: 256m
    profiles:
      - data-pipeline
    ports:
      - "5434:5432"
    environment:
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hive
      POSTGRES_DB: metastore
    volumes:
      - postgres-metastore-data:/var/lib/postgresql/data
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U hive"]
      interval: 5s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped

  hive-metastore:
    build:
      context: ../../services/hive-metastore
      dockerfile: Dockerfile
    container_name: rideshare-hive-metastore
    mem_limit: 512m
    profiles:
      - data-pipeline
    ports:
      - "9083:9083"
    environment:
      SERVICE_NAME: metastore
      DB_DRIVER: postgres
      SERVICE_OPTS: >-
        -Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver
        -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres-metastore:5432/metastore
        -Djavax.jdo.option.ConnectionUserName=hive
        -Djavax.jdo.option.ConnectionPassword=hive
    depends_on:
      postgres-metastore:
        condition: service_healthy
      minio:
        condition: service_healthy
    volumes:
      - ../../services/hive-metastore/hive-site.xml:/opt/hive/conf/hive-site.xml:ro
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD-SHELL", "bash -c 'echo > /dev/tcp/localhost/9083' || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 120s
    restart: unless-stopped

  trino:
    image: trinodb/trino:439
    container_name: rideshare-trino
    deploy:
      resources:
        limits:
          memory: 2g
          cpus: "2.0"
        reservations:
          cpus: "0.25"
    profiles:
      - data-pipeline
    ports:
      - "8084:8080"
    environment:
      - TRINO_ENVIRONMENT=docker
    volumes:
      - ../../services/trino/etc:/etc/trino:ro
    depends_on:
      hive-metastore:
        condition: service_healthy
      minio:
        condition: service_healthy
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/v1/info"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 60s
    restart: unless-stopped

  prometheus:
    image: prom/prometheus:v3.9.1
    container_name: rideshare-prometheus
    mem_limit: 512m
    profiles:
      - monitoring
    ports:
      - "9090:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=7d'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
      - '--web.enable-remote-write-receiver'
    volumes:
      - ../../services/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ../../services/prometheus/rules:/etc/prometheus/rules:ro
      - prometheus-data:/prometheus
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  cadvisor:
    image: ghcr.io/google/cadvisor:v0.53.0
    container_name: rideshare-cadvisor
    mem_limit: 256m
    profiles:
      - monitoring
    ports:
      - "8083:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    devices:
      - /dev/kmsg
    networks:
      - rideshare-network
    privileged: true
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8080/healthz"]
      interval: 10s
      timeout: 5s
      retries: 5

  grafana:
    image: grafana/grafana:12.3.1
    container_name: rideshare-grafana
    mem_limit: 384m
    profiles:
      - monitoring
    ports:
      - "3001:3000"
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: admin
      GF_PATHS_PROVISIONING: /etc/grafana/provisioning
      GF_SERVER_ROOT_URL: http://localhost:3001
      GF_AUTH_ANONYMOUS_ENABLED: "false"
      GF_INSTALL_PLUGINS: trino-datasource
    volumes:
      - ../../services/grafana/provisioning/datasources:/etc/grafana/provisioning/datasources:ro
      - ../../services/grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ../../services/grafana/dashboards:/etc/dashboards:ro
      - ../../services/grafana/provisioning/alerting:/etc/grafana/provisioning/alerting:ro
      - grafana-data:/var/lib/grafana
    depends_on:
      prometheus:
        condition: service_healthy
      loki:
        condition: service_healthy
      tempo:
        condition: service_healthy
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped

  loki:
    image: grafana/loki:3.6.5
    container_name: rideshare-loki
    mem_limit: 256m
    profiles:
      - monitoring
    ports:
      - "3100:3100"
    volumes:
      - ../../services/loki/loki-config.yaml:/etc/loki/local-config.yaml:ro
      - loki-data:/loki
    command: -config.file=/etc/loki/local-config.yaml
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "/usr/bin/loki", "-health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped

  tempo:
    build:
      context: ../../services/tempo
      dockerfile: Dockerfile
    container_name: rideshare-tempo
    mem_limit: 512m
    profiles:
      - monitoring
    ports:
      - "3200:3200"   # Tempo HTTP (query API)
      - "4319:4317"   # OTLP gRPC (from OTel Collector)
    volumes:
      - ../../services/tempo/tempo-config.yaml:/etc/tempo/tempo.yaml:ro
      - tempo-data:/var/tempo
    command: ["-config.file=/etc/tempo/tempo.yaml"]
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3200/ready"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped

  otel-collector:
    build:
      context: ../../services/otel-collector
      dockerfile: Dockerfile
    container_name: rideshare-otel-collector
    user: "0:0"
    mem_limit: 768m
    profiles:
      - monitoring
    ports:
      - "4317:4317"   # OTLP gRPC receiver
      - "4318:4318"   # OTLP HTTP receiver
      - "8888:8888"   # Collector metrics
    volumes:
      - ../../services/otel-collector/otel-collector-config.yaml:/etc/otelcol-contrib/config.yaml:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    command: ["--config=/etc/otelcol-contrib/config.yaml"]
    depends_on:
      prometheus:
        condition: service_healthy
      loki:
        condition: service_healthy
      tempo:
        condition: service_healthy
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:13133/"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped

networks:
  rideshare-network:
    driver: bridge
    name: rideshare-network

volumes:
  kafka-data:
  redis-data:
  osrm-data:
  simulation-db:
  frontend-node-modules:
  minio-data:
  localstack-data:
  postgres-airflow-data:
  postgres-metastore-data:
  prometheus-data:
  grafana-data:
  loki-data:
  tempo-data:
