name: rideshare-platform

# Docker Compose Profiles:
# - core: Simulation runtime services (kafka, redis, osrm, simulation, stream-processor, frontend)
# - data-pipeline: ETL, streaming, and orchestration (minio, spark-*, localstack, airflow)
#   (Consolidated from data-platform + quality-orchestration on 2026-01-26)
# - monitoring: Observability (prometheus, cadvisor, grafana)
# - bi: Business intelligence (superset, postgres-superset, redis-superset)

services:
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: rideshare-kafka
    mem_limit: 1g
    profiles:
      - core
      - data-pipeline
    ports:
      - "9092:9092"
    environment:
      # JVM flags for ARM/Apple Silicon stability
      KAFKA_OPTS: "-XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent"
      KAFKA_HEAP_OPTS: "-Xms256m -Xmx512m"

      # Log retention - auto-delete since no consumers yet
      KAFKA_LOG_RETENTION_HOURS: 1
      KAFKA_LOG_RETENTION_BYTES: 536870912  # 512MB per partition

      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_LISTENERS: PLAINTEXT://kafka:29092,CONTROLLER://kafka:29093,PLAINTEXT_HOST://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:29093
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_LOG_DIRS: /tmp/kraft-combined-logs
      CLUSTER_ID: MkU3OEVBNTcwNTJENDM2Qk
    volumes:
      - kafka-data:/var/lib/kafka/data
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 10s
      timeout: 10s
      retries: 10

  kafka-init:
    image: confluentinc/cp-kafka:7.5.0
    container_name: rideshare-kafka-init
    profiles:
      - core
      - data-pipeline
    depends_on:
      kafka:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      echo 'Creating Kafka topics...' &&
      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic trips --partitions 4 --replication-factor 1 &&
      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic gps-pings --partitions 8 --replication-factor 1 &&
      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic driver-status --partitions 2 --replication-factor 1 &&
      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic surge-updates --partitions 2 --replication-factor 1 &&
      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic ratings --partitions 2 --replication-factor 1 &&
      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic payments --partitions 2 --replication-factor 1 &&
      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic driver-profiles --partitions 1 --replication-factor 1 &&
      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic rider-profiles --partitions 1 --replication-factor 1 &&
      echo 'Kafka topics created successfully' &&
      kafka-topics --bootstrap-server kafka:29092 --list
      "
    networks:
      - rideshare-network

  schema-registry:
    image: confluentinc/cp-schema-registry:7.5.0
    container_name: rideshare-schema-registry
    mem_limit: 512m
    profiles:
      - core
      - data-pipeline
    ports:
      - "8085:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka:29092
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
      SCHEMA_REGISTRY_HEAP_OPTS: "-Xms128m -Xmx256m"
    depends_on:
      kafka:
        condition: service_healthy
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/subjects"]
      interval: 10s
      timeout: 10s
      retries: 12
      start_period: 30s

  redis:
    image: redis:8.0-alpine
    container_name: rideshare-redis
    mem_limit: 128m
    profiles:
      - core
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5

  osrm:
    build:
      context: ../..
      dockerfile: infrastructure/docker/dockerfiles/osrm.Dockerfile
      args:
        OSRM_MAP_SOURCE: ${OSRM_MAP_SOURCE:-local}
    container_name: rideshare-osrm
    mem_limit: 1g
    profiles:
      - core
    platform: linux/amd64
    ports:
      - "5050:5000"
    environment:
      - OSRM_THREADS=${OSRM_THREADS:-4}
    volumes:
      - osrm-data:/data
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:5000/route/v1/driving/-46.6333,-23.5505;-46.6334,-23.5506"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 180s

  simulation:
    build:
      context: ../../services/simulation
      dockerfile: Dockerfile
      target: development
    container_name: rideshare-simulation
    mem_limit: 1g
    profiles:
      - core
    ports:
      - "8000:8000"
    volumes:
      - ../../services/simulation/src:/app/src:ro
      - ../../data:/app/data:ro
      - simulation-db:/app/db
    environment:
      - SIM_SPEED_MULTIPLIER=${SIM_SPEED_MULTIPLIER:-1}
      - SIM_LOG_LEVEL=${SIM_LOG_LEVEL:-INFO}
      - SIM_CHECKPOINT_INTERVAL=${SIM_CHECKPOINT_INTERVAL:-300}
      - SIM_DB_PATH=/app/db/simulation.db
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
      - KAFKA_SECURITY_PROTOCOL=PLAINTEXT
      - KAFKA_SCHEMA_REGISTRY_URL=http://schema-registry:8081
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=
      - REDIS_SSL=false
      - OSRM_BASE_URL=http://osrm:5000
      - API_KEY=${API_KEY:-dev-api-key-change-in-production}
      - CORS_ORIGINS=http://localhost:3000,http://localhost:5173,http://localhost:5174,http://localhost:5175,http://localhost:5176,http://localhost:5177,http://localhost:5178
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
      redis:
        condition: service_healthy
      osrm:
        condition: service_healthy
      stream-processor:
        condition: service_healthy
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 5

  stream-processor:
    build:
      context: ../../services/stream-processor
      dockerfile: Dockerfile
    container_name: rideshare-stream-processor
    mem_limit: 256m
    profiles:
      - core
    ports:
      - "8080:8080"
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
      - KAFKA_GROUP_ID=stream-processor
      - KAFKA_AUTO_OFFSET_RESET=latest
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=
      - PROCESSOR_WINDOW_SIZE_MS=${PROCESSOR_WINDOW_SIZE_MS:-100}
      - PROCESSOR_AGGREGATION_STRATEGY=${PROCESSOR_AGGREGATION_STRATEGY:-latest}
      - PROCESSOR_TOPICS=gps-pings,trips,driver-status,surge-updates
      - API_HOST=0.0.0.0
      - API_PORT=8080
      - LOG_LEVEL=${PROCESSOR_LOG_LEVEL:-INFO}
    depends_on:
      kafka:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  control-panel-frontend:
    build:
      context: ../../services/frontend
      dockerfile: Dockerfile
      target: development
    container_name: rideshare-frontend
    mem_limit: 384m
    profiles:
      - core
    ports:
      - "5173:5173"
    volumes:
      - ../../services/frontend:/app
      - frontend-node-modules:/app/node_modules
    environment:
      - VITE_API_URL=http://localhost:8000
      - VITE_WS_URL=ws://localhost:8000/ws
    depends_on:
      simulation:
        condition: service_healthy
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "node", "-e", "require('http').get('http://localhost:5173', (r) => process.exit(r.statusCode === 200 ? 0 : 1)).on('error', () => process.exit(1))"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  minio:
    build:
      context: ../..
      dockerfile: infrastructure/docker/dockerfiles/minio.Dockerfile
      args:
        MINIO_RELEASE: RELEASE.2025-10-15T17-29-55Z
    container_name: rideshare-minio
    mem_limit: 256m
    profiles:
      - data-pipeline
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001"
    volumes:
      - minio-data:/data
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 10s
      timeout: 5s
      retries: 5

  minio-init:
    image: quay.io/minio/mc:RELEASE.2025-08-13T08-35-41Z
    container_name: rideshare-minio-init
    profiles:
      - data-pipeline
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      mc alias set local http://minio:9000 minioadmin minioadmin &&
      mc mb local/rideshare-bronze --ignore-existing &&
      mc mb local/rideshare-silver --ignore-existing &&
      mc mb local/rideshare-gold --ignore-existing &&
      mc mb local/rideshare-checkpoints --ignore-existing &&
      echo 'MinIO buckets initialized successfully'
      "
    networks:
      - rideshare-network

  # ============================================================================
  # COMMENTED OUT: Spark Cluster Mode (migrated to local mode 2026-01-19)
  # Each Spark service now runs in local mode within its container.
  # Preserved for reference - uncomment to restore cluster mode if needed.
  # ============================================================================
  # spark-master:
  #   build:
  #     context: ../..
  #     dockerfile: infrastructure/docker/dockerfiles/spark-delta.Dockerfile
  #   container_name: rideshare-spark-master
  #   mem_limit: 512m
  #   profiles:
  #     - data-platform
  #   ports:
  #     - "7077:7077"
  #     - "4040:8080"
  #   command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
  #   environment:
  #     - PYTHONPATH=/opt
  #   volumes:
  #     - ../../services/spark-streaming:/opt/spark_streaming:ro
  #   networks:
  #     - rideshare-network
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:8080"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 10
  #     start_period: 30s
  #
  # spark-worker:
  #   build:
  #     context: ../..
  #     dockerfile: infrastructure/docker/dockerfiles/spark-delta.Dockerfile
  #   container_name: rideshare-spark-worker
  #   mem_limit: 3g
  #   profiles:
  #     - data-platform
  #   ports:
  #     - "8081:8081"
  #   command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker -m 2g -c 4 --webui-port 8081 spark://spark-master:7077
  #   environment:
  #     - PYTHONPATH=/opt
  #   volumes:
  #     - ../../services/spark-streaming:/opt/spark_streaming:ro
  #   depends_on:
  #     spark-master:
  #       condition: service_healthy
  #     minio:
  #       condition: service_healthy
  #   networks:
  #     - rideshare-network
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:8081"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 10
  #     start_period: 30s

  spark-thrift-server:
    build:
      context: ../..
      dockerfile: infrastructure/docker/dockerfiles/spark-delta.Dockerfile
    container_name: rideshare-spark-thrift-server
    mem_limit: 1024m
    profiles:
      - data-pipeline
    ports:
      - "10000:10000"
      - "4041:4040"
    command: >
      /opt/spark/bin/spark-submit
      --master local[2]
      --driver-memory 512m
      --conf spark.driver.maxResultSize=256m
      --class org.apache.spark.sql.hive.thriftserver.HiveThriftServer2
      --name "Thrift JDBC/ODBC Server"
      --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
      --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog
      --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000
      --conf spark.hadoop.fs.s3a.access.key=minioadmin
      --conf spark.hadoop.fs.s3a.secret.key=minioadmin
      --conf spark.hadoop.fs.s3a.path.style.access=true
      --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false
      --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
      --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
      --conf spark.sql.warehouse.dir=s3a://rideshare-silver/warehouse/
      --hiveconf hive.server2.thrift.port=10000
      --hiveconf hive.server2.thrift.bind.host=0.0.0.0
      --hiveconf hive.server2.authentication=NOSASL
      --hiveconf hive.server2.transport.mode=binary
    environment:
      - PYTHONPATH=/opt
    volumes:
      - ../../services/spark-streaming:/opt/spark_streaming:ro
    depends_on:
      minio:
        condition: service_healthy
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:4040/json/ > /dev/null || exit 1"]
      interval: 10s
      timeout: 10s
      retries: 10
      start_period: 60s

  spark-streaming-high-volume:
    build:
      context: ../..
      dockerfile: infrastructure/docker/dockerfiles/spark-delta.Dockerfile
    container_name: rideshare-spark-streaming-high-volume
    mem_limit: 768m
    profiles:
      - data-pipeline
    environment:
      - PYTHONPATH=/opt
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
      - SCHEMA_REGISTRY_URL=http://schema-registry:8081
      - KAFKA_TOPICS=gps-pings
      - CHECKPOINT_BASE_PATH=s3a://rideshare-checkpoints/
      - TRIGGER_INTERVAL=10 seconds
    volumes:
      - ../../services/spark-streaming:/opt/spark_streaming:ro
    depends_on:
      minio:
        condition: service_healthy
      kafka:
        condition: service_healthy
    networks:
      - rideshare-network
    command: >
      /opt/spark/bin/spark-submit
      --master local[2]
      --driver-memory 512m
      --name "streaming_high_volume"
      --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
      --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog
      --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000
      --conf spark.hadoop.fs.s3a.access.key=minioadmin
      --conf spark.hadoop.fs.s3a.secret.key=minioadmin
      --conf spark.hadoop.fs.s3a.path.style.access=true
      /opt/spark_streaming/jobs/high_volume_streaming_job.py
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f 'spark-submit' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  spark-streaming-low-volume:
    build:
      context: ../..
      dockerfile: infrastructure/docker/dockerfiles/spark-delta.Dockerfile
    container_name: rideshare-spark-streaming-low-volume
    mem_limit: 768m
    profiles:
      - data-pipeline
    environment:
      - PYTHONPATH=/opt
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
      - SCHEMA_REGISTRY_URL=http://schema-registry:8081
      - KAFKA_TOPICS=trips,driver-status,surge-updates,ratings,payments,driver-profiles,rider-profiles
      - CHECKPOINT_BASE_PATH=s3a://rideshare-checkpoints/
      - TRIGGER_INTERVAL=10 seconds
    volumes:
      - ../../services/spark-streaming:/opt/spark_streaming:ro
    depends_on:
      minio:
        condition: service_healthy
      kafka:
        condition: service_healthy
    networks:
      - rideshare-network
    command: >
      /opt/spark/bin/spark-submit
      --master local[2]
      --driver-memory 512m
      --name "streaming_low_volume"
      --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
      --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog
      --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000
      --conf spark.hadoop.fs.s3a.access.key=minioadmin
      --conf spark.hadoop.fs.s3a.secret.key=minioadmin
      --conf spark.hadoop.fs.s3a.path.style.access=true
      /opt/spark_streaming/jobs/low_volume_streaming_job.py
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f 'spark-submit' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  bronze-init:
    image: apache/airflow:3.1.5
    container_name: rideshare-bronze-init
    profiles:
      - data-pipeline
    environment:
      - _PIP_ADDITIONAL_REQUIREMENTS=pyhive thrift
    volumes:
      - ../../infrastructure/scripts:/opt/init-scripts:ro
    depends_on:
      spark-thrift-server:
        condition: service_healthy
    networks:
      - rideshare-network
    command: bash /opt/init-scripts/wait-for-thrift-and-init-bronze.sh
    restart: "no"

  localstack:
    image: localstack/localstack:4.12.0
    container_name: rideshare-localstack
    mem_limit: 384m
    profiles:
      - data-pipeline
    ports:
      - "4566:4566"
      - "4510-4559:4510-4559"
    environment:
      - SERVICES=secretsmanager,sns,sqs
      - PERSISTENCE=0
      - LOCALSTACK_HOST=localhost:4566
      - DEBUG=0
      - LS_LOG=info
    volumes:
      - localstack-data:/var/lib/localstack
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4566/_localstack/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  postgres-airflow:
    image: postgres:16
    container_name: rideshare-postgres-airflow
    mem_limit: 256m
    profiles:
      - data-pipeline
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-airflow-data:/var/lib/postgresql/data
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 5s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  airflow-webserver:
    image: apache/airflow:3.1.5
    container_name: rideshare-airflow-webserver
    mem_limit: 384m
    profiles:
      - data-pipeline
    ports:
      - "8082:8080"
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres-airflow/airflow
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__CORE__AUTH_MANAGER: 'airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager'
      AIRFLOW__CORE__PARALLELISM: 8
      AIRFLOW__DAG_PROCESSOR__REFRESH_INTERVAL: 30
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: admin
      _AIRFLOW_WWW_USER_PASSWORD: admin
      _PIP_ADDITIONAL_REQUIREMENTS: 'apache-airflow-providers-fab apache-airflow-providers-apache-spark pyhive thrift dbt-core dbt-spark[PyHive]'
    volumes:
      - ../../services/airflow/dags:/opt/airflow/dags
      - ../../services/airflow/logs:/opt/airflow/logs
      - ../../services/airflow/plugins:/opt/airflow/plugins
      - ../../services/airflow/config:/opt/airflow/config
      - ../../services/dbt:/opt/dbt
      - ../../quality/great-expectations:/opt/great-expectations
    depends_on:
      postgres-airflow:
        condition: service_healthy
    networks:
      - rideshare-network
    command: api-server
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8080/api/v2/monitor/health', timeout=3)"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 300s
    restart: unless-stopped

  airflow-scheduler:
    image: apache/airflow:3.1.5
    container_name: rideshare-airflow-scheduler
    mem_limit: 384m
    profiles:
      - data-pipeline
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres-airflow/airflow
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__CORE__AUTH_MANAGER: 'airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager'
      AIRFLOW__CORE__PARALLELISM: 8
      AIRFLOW__DAG_PROCESSOR__REFRESH_INTERVAL: 30
      _PIP_ADDITIONAL_REQUIREMENTS: 'apache-airflow-providers-fab apache-airflow-providers-apache-spark pyhive thrift dbt-core dbt-spark[PyHive]'
    volumes:
      - ../../services/airflow/dags:/opt/airflow/dags
      - ../../services/airflow/logs:/opt/airflow/logs
      - ../../services/airflow/plugins:/opt/airflow/plugins
      - ../../services/airflow/config:/opt/airflow/config
      - ../../services/dbt:/opt/dbt
      - ../../quality/great-expectations:/opt/great-expectations
      - ../../infrastructure/scripts:/opt/init-scripts:ro
    depends_on:
      postgres-airflow:
        condition: service_healthy
      airflow-webserver:
        condition: service_healthy  # Ensure DB migrations complete before reserialization
      spark-thrift-server:
        condition: service_healthy  # Required for DBT tasks to connect
    networks:
      - rideshare-network
    # Reserialize DAGs on startup to ensure they're loaded in Airflow 3.x
    command: bash -c "airflow dags reserialize && exec airflow scheduler"
    healthcheck:
      test: ["CMD", "python", "-c", "import socket; s = socket.socket(); s.settimeout(3); s.connect(('localhost', 8793)); s.close()"]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped

  prometheus:
    image: prom/prometheus:v3.9.1
    container_name: rideshare-prometheus
    mem_limit: 512m
    profiles:
      - monitoring
    ports:
      - "9090:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=7d'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
    volumes:
      - ../../infrastructure/monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ../../infrastructure/monitoring/prometheus/rules:/etc/prometheus/rules:ro
      - prometheus-data:/prometheus
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  cadvisor:
    image: ghcr.io/google/cadvisor:v0.53.0
    container_name: rideshare-cadvisor
    mem_limit: 256m
    profiles:
      - monitoring
    ports:
      - "8083:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    devices:
      - /dev/kmsg
    networks:
      - rideshare-network
    privileged: true
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8080/healthz"]
      interval: 10s
      timeout: 5s
      retries: 5

  grafana:
    image: grafana/grafana:12.3.1
    container_name: rideshare-grafana
    mem_limit: 192m
    profiles:
      - monitoring
    ports:
      - "3001:3000"
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: admin
      GF_PATHS_PROVISIONING: /etc/grafana/provisioning
      GF_SERVER_ROOT_URL: http://localhost:3001
      GF_AUTH_ANONYMOUS_ENABLED: "false"
    volumes:
      - ../../infrastructure/monitoring/grafana/provisioning/datasources:/etc/grafana/provisioning/datasources:ro
      - ../../infrastructure/monitoring/grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ../../infrastructure/monitoring/grafana/dashboards:/etc/dashboards:ro
      - ../../infrastructure/monitoring/grafana/provisioning/alerting:/etc/grafana/provisioning/alerting:ro
      - grafana-data:/var/lib/grafana
    depends_on:
      prometheus:
        condition: service_healthy
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped

  postgres-superset:
    image: postgres:16
    container_name: rideshare-postgres-superset
    mem_limit: 256m
    profiles:
      - analytics
    ports:
      - "5433:5432"
    environment:
      POSTGRES_USER: superset
      POSTGRES_PASSWORD: superset
      POSTGRES_DB: superset
    volumes:
      - postgres-superset-data:/var/lib/postgresql/data
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U superset"]
      interval: 5s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  redis-superset:
    image: redis:8.0-alpine
    container_name: rideshare-redis-superset
    mem_limit: 128m
    profiles:
      - analytics
    ports:
      - "6380:6379"
    volumes:
      - redis-superset-data:/data
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5
    restart: unless-stopped

  superset-init:
    image: apache/superset:6.0.0
    container_name: rideshare-superset-init
    user: root
    profiles:
      - analytics
    environment:
      SUPERSET_SECRET_KEY: dev-secret-key-change-in-production
      PYTHONPATH: /app/pythonpath
    volumes:
      - ../../analytics/superset/superset_config.py:/app/pythonpath/superset_config.py:ro
    depends_on:
      postgres-superset:
        condition: service_healthy
      redis-superset:
        condition: service_healthy
    networks:
      - rideshare-network
    command: >
      /bin/sh -c "
      echo 'Installing dependencies to Superset venv...' &&
      pip install --target=/app/.venv/lib/python3.10/site-packages pyhive thrift psycopg2-binary &&

      echo 'Running database migrations...' &&
      superset db upgrade &&

      echo 'Waiting for migrations to settle...' &&
      sleep 5 &&

      echo 'Creating admin user...' &&
      superset fab create-admin --username admin --firstname Admin --lastname User --email admin@superset.com --password admin || true &&

      echo 'Initializing Superset (with retry)...' &&
      for i in 1 2 3 4 5; do
        superset init && break || {
          echo \"Init attempt $$i failed, retrying in 5 seconds...\";
          sleep 5;
        };
      done &&

      echo 'Superset initialization complete!'
      "

  superset:
    image: apache/superset:6.0.0
    container_name: rideshare-superset
    mem_limit: 768m
    profiles:
      - analytics
    ports:
      - "8088:8088"
    user: root
    environment:
      SUPERSET_SECRET_KEY: dev-secret-key-change-in-production
      PYTHONPATH: /tmp/python-packages:/app/pythonpath
    volumes:
      - ../../analytics/superset/superset_config.py:/app/pythonpath/superset_config.py:ro
      - ../../analytics/superset/docker-entrypoint.sh:/app/docker-entrypoint.sh:ro
    depends_on:
      superset-init:
        condition: service_completed_successfully
    networks:
      - rideshare-network
    command: ["/app/docker-entrypoint.sh"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8088/health"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 60s
    restart: unless-stopped

networks:
  rideshare-network:
    driver: bridge
    name: rideshare-network

volumes:
  kafka-data:
  redis-data:
  osrm-data:
  simulation-db:
  frontend-node-modules:
  minio-data:
  localstack-data:
  postgres-airflow-data:
  postgres-superset-data:
  redis-superset-data:
  prometheus-data:
  grafana-data:
