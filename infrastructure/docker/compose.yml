services:
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: rideshare-kafka
    mem_limit: 1g
    profiles:
      - core
    ports:
      - "9092:9092"
    environment:
      # JVM flags for ARM/Apple Silicon stability
      KAFKA_OPTS: "-XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent"
      KAFKA_HEAP_OPTS: "-Xms256m -Xmx512m"

      # Log retention - auto-delete since no consumers yet
      KAFKA_LOG_RETENTION_HOURS: 1
      KAFKA_LOG_RETENTION_BYTES: 536870912  # 512MB per partition

      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_LISTENERS: PLAINTEXT://kafka:29092,CONTROLLER://kafka:29093,PLAINTEXT_HOST://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:29093
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_LOG_DIRS: /tmp/kraft-combined-logs
      CLUSTER_ID: MkU3OEVBNTcwNTJENDM2Qk
    volumes:
      - kafka-data:/var/lib/kafka/data
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 10s
      timeout: 10s
      retries: 10

  kafka-init:
    image: confluentinc/cp-kafka:7.5.0
    container_name: rideshare-kafka-init
    profiles:
      - core
    depends_on:
      kafka:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      echo 'Creating Kafka topics...' &&
      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic trips --partitions 4 --replication-factor 1 &&
      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic gps-pings --partitions 8 --replication-factor 1 &&
      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic driver-status --partitions 2 --replication-factor 1 &&
      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic surge-updates --partitions 2 --replication-factor 1 &&
      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic ratings --partitions 2 --replication-factor 1 &&
      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic payments --partitions 2 --replication-factor 1 &&
      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic driver-profiles --partitions 1 --replication-factor 1 &&
      kafka-topics --bootstrap-server kafka:29092 --create --if-not-exists --topic rider-profiles --partitions 1 --replication-factor 1 &&
      echo 'Kafka topics created successfully' &&
      kafka-topics --bootstrap-server kafka:29092 --list
      "
    networks:
      - rideshare-network

  schema-registry:
    image: confluentinc/cp-schema-registry:7.5.0
    container_name: rideshare-schema-registry
    mem_limit: 512m
    profiles:
      - core
    ports:
      - "8085:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka:29092
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
    depends_on:
      kafka:
        condition: service_healthy
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/subjects"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:8.0-alpine
    container_name: rideshare-redis
    mem_limit: 512m
    profiles:
      - core
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5

  osrm:
    build:
      context: ../..
      dockerfile: infrastructure/docker/dockerfiles/osrm.Dockerfile
      args:
        OSRM_MAP_SOURCE: ${OSRM_MAP_SOURCE:-local}
    container_name: rideshare-osrm
    mem_limit: 3g
    profiles:
      - core
    platform: linux/amd64
    ports:
      - "5050:5000"
    environment:
      - OSRM_THREADS=${OSRM_THREADS:-4}
    volumes:
      - osrm-data:/data
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:5000/route/v1/driving/-46.6333,-23.5505;-46.6334,-23.5506"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 180s

  simulation:
    build:
      context: ../../services/simulation
      dockerfile: Dockerfile
      target: development
    container_name: rideshare-simulation
    mem_limit: 4g
    profiles:
      - core
    ports:
      - "8000:8000"
    volumes:
      - ../../services/simulation/src:/app/src:ro
      - ../../data:/app/data:ro
      - simulation-db:/app/db
    environment:
      - SIM_SPEED_MULTIPLIER=${SIM_SPEED_MULTIPLIER:-1}
      - SIM_LOG_LEVEL=${SIM_LOG_LEVEL:-INFO}
      - SIM_CHECKPOINT_INTERVAL=${SIM_CHECKPOINT_INTERVAL:-300}
      - SIM_DB_PATH=/app/db/simulation.db
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
      - KAFKA_SECURITY_PROTOCOL=PLAINTEXT
      - KAFKA_SCHEMA_REGISTRY_URL=http://schema-registry:8081
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=
      - REDIS_SSL=false
      - OSRM_BASE_URL=http://osrm:5000
      - API_KEY=${API_KEY:-dev-api-key-change-in-production}
      - CORS_ORIGINS=http://localhost:3000,http://localhost:5173,http://localhost:5174,http://localhost:5175,http://localhost:5176,http://localhost:5177,http://localhost:5178
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
      redis:
        condition: service_healthy
      osrm:
        condition: service_healthy
      stream-processor:
        condition: service_healthy
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 5

  stream-processor:
    build:
      context: ../../services/stream-processor
      dockerfile: Dockerfile
    container_name: rideshare-stream-processor
    mem_limit: 512m
    profiles:
      - core
    ports:
      - "8080:8080"
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
      - KAFKA_GROUP_ID=stream-processor
      - KAFKA_AUTO_OFFSET_RESET=latest
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=
      - PROCESSOR_WINDOW_SIZE_MS=${PROCESSOR_WINDOW_SIZE_MS:-100}
      - PROCESSOR_AGGREGATION_STRATEGY=${PROCESSOR_AGGREGATION_STRATEGY:-latest}
      - PROCESSOR_TOPICS=gps-pings,trips,driver-status,surge-updates
      - API_HOST=0.0.0.0
      - API_PORT=8080
      - LOG_LEVEL=${PROCESSOR_LOG_LEVEL:-INFO}
    depends_on:
      kafka:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:8080/health"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  control-panel-frontend:
    build:
      context: ../../services/frontend
      dockerfile: Dockerfile
      target: development
    container_name: rideshare-frontend
    mem_limit: 512m
    profiles:
      - core
    ports:
      - "3000:5173"
    volumes:
      - ../../services/frontend:/app
      - frontend-node-modules:/app/node_modules
    environment:
      - VITE_API_URL=http://localhost:8000
      - VITE_WS_URL=ws://localhost:8000/ws
    depends_on:
      simulation:
        condition: service_healthy
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:5173"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  minio:
    build:
      context: ../..
      dockerfile: infrastructure/docker/dockerfiles/minio.Dockerfile
      args:
        MINIO_RELEASE: RELEASE.2025-10-15T17-29-55Z
    container_name: rideshare-minio
    mem_limit: 256m
    profiles:
      - data-platform
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001"
    volumes:
      - minio-data:/data
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 10s
      timeout: 5s
      retries: 5

  minio-init:
    image: quay.io/minio/mc:RELEASE.2025-08-13T08-35-41Z
    container_name: rideshare-minio-init
    profiles:
      - data-platform
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      mc alias set local http://minio:9000 minioadmin minioadmin &&
      mc mb local/rideshare-bronze --ignore-existing &&
      mc mb local/rideshare-silver --ignore-existing &&
      mc mb local/rideshare-gold --ignore-existing &&
      mc mb local/rideshare-checkpoints --ignore-existing &&
      echo 'MinIO buckets initialized successfully'
      "
    networks:
      - rideshare-network

  spark-master:
    build:
      context: ../..
      dockerfile: infrastructure/docker/dockerfiles/spark-delta.Dockerfile
    container_name: rideshare-spark-master
    mem_limit: 512m
    profiles:
      - data-platform
    ports:
      - "7077:7077"
      - "4040:8080"
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s

  spark-worker:
    build:
      context: ../..
      dockerfile: infrastructure/docker/dockerfiles/spark-delta.Dockerfile
    container_name: rideshare-spark-worker
    mem_limit: 2g
    profiles:
      - data-platform
    ports:
      - "8081:8081"
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker -m 1536m -c 2 --webui-port 8081 spark://spark-master:7077
    environment:
      - PYTHONPATH=/opt
    volumes:
      - ../../services/spark-streaming:/opt/spark_streaming:ro
    depends_on:
      spark-master:
        condition: service_healthy
      minio:
        condition: service_healthy
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s

  spark-thrift-server:
    build:
      context: ../..
      dockerfile: infrastructure/docker/dockerfiles/spark-delta.Dockerfile
    container_name: rideshare-spark-thrift-server
    mem_limit: 1536m
    profiles:
      - data-platform
    ports:
      - "10000:10000"
      - "4041:4040"
    command: >
      /opt/spark/bin/spark-submit
      --master spark://spark-master:7077
      --class org.apache.spark.sql.hive.thriftserver.HiveThriftServer2
      --name "Thrift JDBC/ODBC Server"
      --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
      --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog
      --conf spark.hadoop.fs.s3a.endpoint=http://minio:9000
      --conf spark.hadoop.fs.s3a.access.key=minioadmin
      --conf spark.hadoop.fs.s3a.secret.key=minioadmin
      --conf spark.hadoop.fs.s3a.path.style.access=true
      --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false
      --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
      --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
      --conf spark.sql.warehouse.dir=s3a://rideshare-silver/warehouse/
      --hiveconf hive.server2.thrift.port=10000
      --hiveconf hive.server2.thrift.bind.host=0.0.0.0
      --hiveconf hive.server2.authentication=NOSASL
      --hiveconf hive.server2.transport.mode=binary
    environment:
      - PYTHONPATH=/opt
    volumes:
      - ../../services/spark-streaming:/opt/spark_streaming:ro
    depends_on:
      spark-master:
        condition: service_healthy
      spark-worker:
        condition: service_healthy
      minio:
        condition: service_healthy
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD-SHELL", "nc -z localhost 10000 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 60s

  localstack:
    image: localstack/localstack:4.12.0
    container_name: rideshare-localstack
    mem_limit: 512m
    profiles:
      - data-platform
    ports:
      - "4566:4566"
      - "4510-4559:4510-4559"
    environment:
      - SERVICES=secretsmanager,sns,sqs
      - PERSISTENCE=0
      - LOCALSTACK_HOST=localhost:4566
      - DEBUG=0
      - LS_LOG=info
    volumes:
      - localstack-data:/var/lib/localstack
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4566/_localstack/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  postgres-airflow:
    image: postgres:16
    container_name: rideshare-postgres-airflow
    mem_limit: 256m
    profiles:
      - quality-orchestration
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-airflow-data:/var/lib/postgresql/data
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow"]
      interval: 5s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  airflow-webserver:
    image: apache/airflow:3.1.5
    container_name: rideshare-airflow-webserver
    mem_limit: 384m
    profiles:
      - quality-orchestration
    ports:
      - "8082:8080"
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres-airflow/airflow
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__CORE__AUTH_MANAGER: 'airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager'
      AIRFLOW__CORE__PARALLELISM: 8
      AIRFLOW__DAG_PROCESSOR__REFRESH_INTERVAL: 30
      # Spark connection for SparkSubmitOperator
      AIRFLOW_CONN_SPARK_DEFAULT: 'spark://spark-master:7077?deploy-mode=client'
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: admin
      _AIRFLOW_WWW_USER_PASSWORD: admin
      _PIP_ADDITIONAL_REQUIREMENTS: 'apache-airflow-providers-fab apache-airflow-providers-apache-spark pyhive thrift'
    volumes:
      - ../../services/airflow/dags:/opt/airflow/dags
      - ../../services/airflow/logs:/opt/airflow/logs
      - ../../services/airflow/plugins:/opt/airflow/plugins
      - ../../services/airflow/config:/opt/airflow/config
      - ../../services/dbt:/opt/dbt
      - ../../quality/great-expectations:/opt/great-expectations
    depends_on:
      postgres-airflow:
        condition: service_healthy
    networks:
      - rideshare-network
    command: api-server
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8080/api/v2/monitor/health', timeout=3)"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 60s
    restart: unless-stopped

  airflow-scheduler:
    image: apache/airflow:3.1.5
    container_name: rideshare-airflow-scheduler
    mem_limit: 384m
    profiles:
      - quality-orchestration
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres-airflow/airflow
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__CORE__AUTH_MANAGER: 'airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager'
      AIRFLOW__CORE__PARALLELISM: 8
      AIRFLOW__DAG_PROCESSOR__REFRESH_INTERVAL: 30
      # Spark connection for SparkSubmitOperator
      AIRFLOW_CONN_SPARK_DEFAULT: 'spark://spark-master:7077?deploy-mode=client'
      _PIP_ADDITIONAL_REQUIREMENTS: 'apache-airflow-providers-fab apache-airflow-providers-apache-spark pyhive thrift'
    volumes:
      - ../../services/airflow/dags:/opt/airflow/dags
      - ../../services/airflow/logs:/opt/airflow/logs
      - ../../services/airflow/plugins:/opt/airflow/plugins
      - ../../services/airflow/config:/opt/airflow/config
      - ../../services/dbt:/opt/dbt
      - ../../quality/great-expectations:/opt/great-expectations
    depends_on:
      postgres-airflow:
        condition: service_healthy
    networks:
      - rideshare-network
    command: scheduler
    healthcheck:
      test: ["CMD", "python", "-c", "import socket; s = socket.socket(); s.settimeout(3); s.connect(('localhost', 8793)); s.close()"]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped

  prometheus:
    image: prom/prometheus:v3.9.1
    container_name: rideshare-prometheus
    mem_limit: 256m
    profiles:
      - monitoring
    ports:
      - "9090:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=7d'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
    volumes:
      - ../../infrastructure/monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ../../infrastructure/monitoring/prometheus/rules:/etc/prometheus/rules:ro
      - prometheus-data:/prometheus
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  cadvisor:
    image: ghcr.io/google/cadvisor:v0.53.0
    container_name: rideshare-cadvisor
    mem_limit: 256m
    profiles:
      - monitoring
    ports:
      - "8083:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    devices:
      - /dev/kmsg
    networks:
      - rideshare-network
    privileged: true
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8080/healthz"]
      interval: 10s
      timeout: 5s
      retries: 5

  postgres-superset:
    image: postgres:16
    container_name: rideshare-postgres-superset
    mem_limit: 256m
    profiles:
      - bi
    ports:
      - "5433:5432"
    environment:
      POSTGRES_USER: superset
      POSTGRES_PASSWORD: superset
      POSTGRES_DB: superset
    volumes:
      - postgres-superset-data:/var/lib/postgresql/data
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U superset"]
      interval: 5s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  redis-superset:
    image: redis:8.0-alpine
    container_name: rideshare-redis-superset
    mem_limit: 128m
    profiles:
      - bi
    ports:
      - "6380:6379"
    volumes:
      - redis-superset-data:/data
    networks:
      - rideshare-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5
    restart: unless-stopped

  superset-init:
    image: apache/superset:6.0.0-dev
    container_name: rideshare-superset-init
    profiles:
      - bi
    environment:
      SUPERSET_SECRET_KEY: dev-secret-key-change-in-production
      PYTHONPATH: /app/pythonpath
    volumes:
      - ../../analytics/superset/superset_config.py:/app/pythonpath/superset_config.py:ro
    depends_on:
      postgres-superset:
        condition: service_healthy
      redis-superset:
        condition: service_healthy
    networks:
      - rideshare-network
    command: >
      /bin/sh -c "
      pip install pyhive thrift &&
      superset db upgrade &&
      superset fab create-admin --username admin --firstname Admin --lastname User --email admin@superset.com --password admin || true &&
      superset init &&
      echo 'Superset initialization complete'
      "

  superset:
    image: apache/superset:6.0.0-dev
    container_name: rideshare-superset
    mem_limit: 768m
    profiles:
      - bi
    ports:
      - "8088:8088"
    user: root
    environment:
      SUPERSET_SECRET_KEY: dev-secret-key-change-in-production
      PYTHONPATH: /tmp/python-packages:/app/pythonpath
    volumes:
      - ../../analytics/superset/superset_config.py:/app/pythonpath/superset_config.py:ro
      - ../../analytics/superset/docker-entrypoint.sh:/app/docker-entrypoint.sh:ro
    depends_on:
      superset-init:
        condition: service_completed_successfully
    networks:
      - rideshare-network
    command: ["/app/docker-entrypoint.sh"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8088/health"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 60s
    restart: unless-stopped

networks:
  rideshare-network:
    driver: bridge
    name: rideshare-network

volumes:
  kafka-data:
  redis-data:
  osrm-data:
  simulation-db:
  frontend-node-modules:
  minio-data:
  localstack-data:
  postgres-airflow-data:
  postgres-superset-data:
  redis-superset-data:
  prometheus-data:
