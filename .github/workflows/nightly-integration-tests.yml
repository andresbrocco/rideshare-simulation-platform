name: Nightly Integration Tests

on:
  schedule:
    - cron: '0 2 * * *'  # Daily at 2 AM UTC
  workflow_dispatch:

jobs:
  check-changes:
    runs-on: ubuntu-latest
    outputs:
      has_changes: ${{ github.event_name == 'workflow_dispatch' || steps.check.outputs.has_changes }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Check for commits in the last 24 hours
        id: check
        run: |
          COMMITS=$(git log --oneline --since="24 hours ago" | wc -l)
          echo "Commits in last 24h: $COMMITS"
          if [ "$COMMITS" -gt 0 ]; then
            echo "has_changes=true" >> "$GITHUB_OUTPUT"
          else
            echo "has_changes=false" >> "$GITHUB_OUTPUT"
          fi

  integration-tests:
    needs: check-changes
    if: needs.check-changes.outputs.has_changes == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 45
    env:
      COMPOSE_FILE: infrastructure/docker/compose.yml
      MINIO_ENDPOINT: http://localhost:9000
      KAFKA_BOOTSTRAP_SERVERS: localhost:9092
      SCHEMA_REGISTRY_URL: http://localhost:8085
      SPARK_THRIFT_HOST: localhost
      SPARK_THRIFT_PORT: 10000
      LOCALSTACK_ENDPOINT: http://localhost:4566
      AWS_ENDPOINT_URL: http://localhost:4566
      AWS_ACCESS_KEY_ID: test
      AWS_SECRET_ACCESS_KEY: test
      AWS_DEFAULT_REGION: us-east-1
      SIM_SPEED_MULTIPLIER: 10
      SIM_LOG_LEVEL: DEBUG

    steps:
      - name: Free up disk space
        run: |
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf /usr/local/lib/android
          sudo rm -rf /opt/ghc
          sudo rm -rf /opt/hostedtoolcache/CodeQL
          sudo apt-get clean
          docker system prune -af --volumes
          df -h

      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          lfs: true

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.13'

      - name: Install test dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e "."

      - name: Install DBT for Silver/Gold layer tests
        run: |
          cd tools/dbt
          python -m venv venv
          ./venv/bin/pip install --upgrade pip
          ./venv/bin/pip install dbt-spark[PyHive]

      - name: Create DBT profiles.yml and install packages
        working-directory: tools/dbt
        run: |
          mkdir -p profiles
          # Template uses env vars loaded from LocalStack in a later step.
          # For dbt deps (no connection needed), write a placeholder first.
          cat > profiles/profiles.yml.tpl << 'EOF'
          rideshare:
            target: dev
            outputs:
              dev:
                type: spark
                method: thrift
                host: localhost
                port: 10000
                user: ${HIVE_LDAP_USERNAME}
                password: ${HIVE_LDAP_PASSWORD}
                schema: default
                auth: LDAP
                threads: 4
                connect_timeout: 60
                connect_retries: 3
          EOF
          sed -i 's/^          //' profiles/profiles.yml.tpl
          # Write a temporary profiles.yml for dbt deps (no connection needed)
          sed "s/\${HIVE_LDAP_USERNAME}/placeholder/;s/\${HIVE_LDAP_PASSWORD}/placeholder/" \
            profiles/profiles.yml.tpl > profiles/profiles.yml
          ./venv/bin/dbt deps --profiles-dir profiles --project-dir .

      - name: Build and start core and data-pipeline services
        run: |
          docker compose -f "$COMPOSE_FILE" --profile core --profile data-pipeline build
          docker builder prune -af
          docker compose -f "$COMPOSE_FILE" --profile core --profile data-pipeline up -d
          echo "Waiting for services to be healthy..."
          sleep 60

      - name: Wait for infrastructure services
        run: |
          echo "Checking MinIO..."
          timeout 120 bash -c 'until curl -s http://localhost:9000/minio/health/live; do sleep 5; done' || true
          echo "Checking Kafka..."
          timeout 120 bash -c "until docker compose -f \"$COMPOSE_FILE\" exec -T kafka kafka-broker-api-versions --bootstrap-server localhost:9092 --command-config /tmp/kafka-client.properties 2>/dev/null; do sleep 5; done" || true
          echo "Checking Redis..."
          timeout 60 bash -c "until docker compose -f \"$COMPOSE_FILE\" exec -T redis sh -c '. /secrets/core.env && redis-cli -a \"\$REDIS_PASSWORD\" ping' 2>/dev/null | grep -q PONG; do sleep 2; done" || true
          echo "Checking LocalStack..."
          timeout 60 bash -c 'until curl -s http://localhost:4566/_localstack/health | grep -q "\"secretsmanager\": \"available\""; do sleep 5; done' || true

      - name: Verify secrets initialization
        run: |
          echo "Checking secrets-init service completed..."
          docker compose -f "$COMPOSE_FILE" logs secrets-init
          docker compose -f "$COMPOSE_FILE" ps -a secrets-init --format json | python3 -c "
          import sys, json
          data = json.loads(sys.stdin.read())
          if isinstance(data, list):
              data = data[0]
          state = data.get('State', '')
          exit_code = data.get('ExitCode', -1)
          print(f'State: {state}, ExitCode: {exit_code}')
          if state != 'exited' or exit_code != 0:
              print('ERROR: secrets-init did not complete successfully')
              sys.exit(1)
          print('secrets-init completed successfully')
          "

      - name: Load credentials from LocalStack
        run: |
          python3 -c "
          import json, os, boto3
          client = boto3.client(
              'secretsmanager',
              endpoint_url='http://localhost:4566',
              aws_access_key_id='test',
              aws_secret_access_key='test',
              region_name='us-east-1',
          )
          # Same key transforms as infrastructure/scripts/fetch-secrets.py
          KEY_TRANSFORMS = {
              'rideshare/postgres-airflow': {'POSTGRES_USER': 'POSTGRES_AIRFLOW_USER', 'POSTGRES_PASSWORD': 'POSTGRES_AIRFLOW_PASSWORD'},
              'rideshare/postgres-metastore': {'POSTGRES_USER': 'POSTGRES_METASTORE_USER', 'POSTGRES_PASSWORD': 'POSTGRES_METASTORE_PASSWORD'},
              'rideshare/grafana': {'ADMIN_USER': 'GF_SECURITY_ADMIN_USER', 'ADMIN_PASSWORD': 'GF_SECURITY_ADMIN_PASSWORD'},
              'rideshare/hive-thrift': {'LDAP_USERNAME': 'HIVE_LDAP_USERNAME', 'LDAP_PASSWORD': 'HIVE_LDAP_PASSWORD'},
          }
          AIRFLOW_MAPPING = {
              'FERNET_KEY': 'AIRFLOW__CORE__FERNET_KEY',
              'INTERNAL_API_SECRET_KEY': 'AIRFLOW__CORE__INTERNAL_API_SECRET_KEY',
              'JWT_SECRET': 'AIRFLOW__API_AUTH__JWT_SECRET',
              'API_SECRET_KEY': 'AIRFLOW__API__SECRET_KEY',
              'ADMIN_USERNAME': 'AIRFLOW_ADMIN_USERNAME',
              'ADMIN_PASSWORD': 'AIRFLOW_ADMIN_PASSWORD',
          }
          secrets = [
              'rideshare/api-key', 'rideshare/minio', 'rideshare/redis',
              'rideshare/kafka', 'rideshare/schema-registry',
              'rideshare/postgres-airflow', 'rideshare/postgres-metastore',
              'rideshare/airflow', 'rideshare/grafana', 'rideshare/hive-thrift',
              'rideshare/ldap',
          ]
          env_file = os.environ['GITHUB_ENV']
          with open(env_file, 'a') as f:
              for name in secrets:
                  resp = client.get_secret_value(SecretId=name)
                  fields = json.loads(resp['SecretString'])
                  if name == 'rideshare/airflow':
                      fields = {AIRFLOW_MAPPING.get(k, k): v for k, v in fields.items()}
                  elif name in KEY_TRANSFORMS:
                      km = KEY_TRANSFORMS[name]
                      fields = {km.get(k, k): v for k, v in fields.items()}
                  for k, v in fields.items():
                      f.write(f'{k}={v}\n')
                      print(f'  Loaded {k}')
          print('All credentials loaded into GITHUB_ENV')
          "

      - name: Render DBT profiles.yml with credentials
        working-directory: tools/dbt
        run: |
          envsubst < profiles/profiles.yml.tpl > profiles/profiles.yml

      - name: Wait for application services
        run: |
          echo "Checking Schema Registry..."
          timeout 120 bash -c "until curl -sf -u \"\$SCHEMA_REGISTRY_USER:\$SCHEMA_REGISTRY_PASSWORD\" http://localhost:8085/subjects; do sleep 5; done" || true
          echo "Checking Simulation API..."
          timeout 120 bash -c 'until curl -s http://localhost:8000/health; do sleep 5; done' || true
          echo "Checking Spark Thrift Server (this may take up to 3 minutes)..."
          timeout 180 bash -c 'until curl -sf http://localhost:4041/json/ 2>/dev/null; do sleep 10; done' || true
          echo "Verifying Spark Thrift LDAP authentication..."
          timeout 60 bash -c "until docker compose -f \"$COMPOSE_FILE\" exec -T spark-thrift-server beeline -u \"jdbc:hive2://localhost:10000/default\" -n \"\$HIVE_LDAP_USERNAME\" -p \"\$HIVE_LDAP_PASSWORD\" -e \"SELECT 1\" 2>/dev/null | grep -q \"1 row selected\"; do sleep 10; done" || true
          echo "Services are ready"

      - name: Run integration tests
        run: |
          python -m pytest tests/integration/ -v --tb=short --junitxml=test-results.xml

      - name: Collect container logs
        if: failure()
        run: |
          mkdir -p logs
          docker compose -f "$COMPOSE_FILE" --profile core --profile data-pipeline logs > logs/container-logs.txt 2>&1

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: nightly-test-results
          path: |
            test-results.xml
            logs/
          retention-days: 30

      - name: Cleanup services
        if: always()
        run: |
          docker compose -f "$COMPOSE_FILE" --profile core --profile data-pipeline down -v --remove-orphans
