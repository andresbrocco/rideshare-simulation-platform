name: Deploy

on:
  workflow_dispatch:
    inputs:
      action:
        description: 'What to deploy'
        required: true
        type: choice
        options:
          - deploy-all
          - deploy-platform
          - deploy-frontend
          - build-images

permissions:
  id-token: write
  contents: write

jobs:
  build-images:
    if: contains(fromJSON('["deploy-all", "build-images"]'), github.event.inputs.action)
    runs-on: ubuntu-latest
    timeout-minutes: 30

    strategy:
      matrix:
        include:
          - service: simulation
            target: production
          - service: stream-processor
          - service: control-panel
            target: production
          - service: bronze-ingestion
          - service: hive-metastore
          - service: performance-controller
          - service: osrm
          - service: otel-collector
      fail-fast: false

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          lfs: true

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/rideshare-github-actions
          aws-region: us-east-1

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build and push ${{ matrix.service }} image
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          ECR_REPOSITORY: rideshare/${{ matrix.service }}
          IMAGE_TAG: ${{ github.sha }}
          LAMBDA_URL: ${{ vars.LAMBDA_URL }}
        run: |
          # Simulation needs the repo-root schemas/ directory inside its build context
          if [ "${{ matrix.service }}" = "simulation" ]; then
            cp -r schemas services/simulation/schemas
          fi

          BUILD_ARGS=(
            --file "services/${{ matrix.service }}/Dockerfile"
            --tag "$ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG"
            --tag "$ECR_REGISTRY/$ECR_REPOSITORY:latest"
            --push
          )

          if [ -n "${{ matrix.target }}" ]; then
            BUILD_ARGS+=(--target "${{ matrix.target }}")
          fi

          # Control panel needs production URLs baked into the Vite bundle
          if [ "${{ matrix.service }}" = "control-panel" ]; then
            BUILD_ARGS+=(
              --build-arg "VITE_API_URL=https://api.ridesharing.portfolio.andresbrocco.com"
              --build-arg "VITE_WS_URL=wss://api.ridesharing.portfolio.andresbrocco.com/ws"
              --build-arg "VITE_LAMBDA_URL=${LAMBDA_URL}"
            )
          fi

          docker buildx build "${BUILD_ARGS[@]}" "services/${{ matrix.service }}"

          if [ "${{ matrix.service }}" = "simulation" ]; then
            rm -rf services/simulation/schemas
          fi

      - name: Output image URIs
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          ECR_REPOSITORY: rideshare/${{ matrix.service }}
        run: |
          echo "Pushed ${{ matrix.service }} images:"
          echo "  SHA tag:    $ECR_REGISTRY/$ECR_REPOSITORY:${{ github.sha }}"
          echo "  Latest tag: $ECR_REGISTRY/$ECR_REPOSITORY:latest"

  deploy-platform:
    if: always() && !failure() && !cancelled() && contains(fromJSON('["deploy-all", "deploy-platform"]'), github.event.inputs.action)
    needs: [build-images]
    runs-on: ubuntu-latest
    timeout-minutes: 45

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.14.3

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/rideshare-github-actions
          aws-region: us-east-1

      - name: Terraform Init (Platform)
        working-directory: infrastructure/terraform/platform
        run: |
          terraform init \
            -backend-config="bucket=rideshare-tf-state-${{ secrets.AWS_ACCOUNT_ID }}"

      - name: Reconcile Terraform State
        working-directory: infrastructure/terraform/platform
        run: |
          echo "Importing orphaned resources into Terraform state (safe to fail)..."

          # EKS addons (may already exist from a previous partial apply)
          terraform import 'module.eks.aws_eks_addon.ebs_csi' 'rideshare-eks:aws-ebs-csi-driver' 2>&1 || true
          terraform import 'module.eks.aws_eks_addon.coredns' 'rideshare-eks:coredns' 2>&1 || true
          terraform import 'module.eks.aws_eks_addon.kube_proxy' 'rideshare-eks:kube-proxy' 2>&1 || true
          terraform import 'module.eks.aws_eks_addon.vpc_cni' 'rideshare-eks:vpc-cni' 2>&1 || true

          # Helm releases (may already exist from a previous partial apply)
          terraform import 'module.alb.helm_release.aws_load_balancer_controller' 'kube-system/aws-load-balancer-controller' 2>&1 || true
          terraform import 'module.alb.helm_release.external_secrets' 'external-secrets/external-secrets' 2>&1 || true
          terraform import 'module.alb.helm_release.kube_state_metrics' 'monitoring/kube-state-metrics' 2>&1 || true

          echo "State reconciliation complete."

      - name: Terraform Plan (Platform)
        working-directory: infrastructure/terraform/platform
        run: terraform plan -out=tfplan

      - name: Terraform Apply (Platform)
        working-directory: infrastructure/terraform/platform
        run: terraform apply -auto-approve tfplan

      - name: Get Terraform Outputs
        id: tf-outputs
        working-directory: infrastructure/terraform/platform
        run: |
          CLUSTER_NAME=$(terraform output -raw cluster_name)
          CLUSTER_ENDPOINT=$(terraform output -raw cluster_endpoint)
          RDS_ENDPOINT=$(terraform output -raw rds_endpoint)
          {
            echo "cluster_name=$CLUSTER_NAME"
            echo "cluster_endpoint=$CLUSTER_ENDPOINT"
            echo "rds_endpoint=$RDS_ENDPOINT"
          } >> "$GITHUB_OUTPUT"

      - name: Resolve Production Values
        run: |
          ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          RDS_ENDPOINT="${{ steps.tf-outputs.outputs.rds_endpoint }}"
          ACM_CERT_ARN=$(aws acm list-certificates --region us-east-1 \
            --query "CertificateSummaryList[?DomainName=='ridesharing.portfolio.andresbrocco.com'].CertificateArn" \
            --output text)
          ALB_SG_ID=$(aws ec2 describe-security-groups --region us-east-1 \
            --filters "Name=group-name,Values=rideshare-alb-sg" \
            --query 'SecurityGroups[0].GroupId' --output text)

          echo "Account ID: $ACCOUNT_ID"
          echo "RDS Endpoint: $RDS_ENDPOINT"
          echo "ACM Cert ARN: $ACM_CERT_ARN"
          echo "ALB SG ID: $ALB_SG_ID"

          if [ -z "$ACCOUNT_ID" ] || [ -z "$RDS_ENDPOINT" ] || [ -z "$ACM_CERT_ARN" ] || [ -z "$ALB_SG_ID" ]; then
            echo "Error: One or more production values could not be resolved"
            exit 1
          fi

          IMAGE_TAG="${{ github.sha }}"

          # Replace all placeholders in production overlay
          find infrastructure/kubernetes/overlays/production -name "*.yaml" \
            -exec sed -i "s|<account-id>|$ACCOUNT_ID|g" {} +
          find infrastructure/kubernetes/overlays/production -name "*.yaml" \
            -exec sed -i "s|<acm-cert-arn>|$ACM_CERT_ARN|g" {} +
          find infrastructure/kubernetes/overlays/production -name "*.yaml" \
            -exec sed -i "s|<rds-endpoint>|$RDS_ENDPOINT|g" {} +
          find infrastructure/kubernetes/overlays/production -name "*.yaml" \
            -exec sed -i "s|<image-tag>|$IMAGE_TAG|g" {} +
          find infrastructure/kubernetes/overlays/production -name "*.yaml" \
            -exec sed -i "s|<alb-sg-id>|$ALB_SG_ID|g" {} +

          # Verify no placeholders remain
          REMAINING=$(grep -r '<account-id>\|<acm-cert-arn>\|<rds-endpoint>\|<image-tag>\|<alb-sg-id>' \
            infrastructure/kubernetes/overlays/production/ || true)
          if [ -n "$REMAINING" ]; then
            echo "Error: Unresolved placeholders remain:"
            echo "$REMAINING"
            exit 1
          fi
          echo "All placeholders resolved successfully"

      - name: Package dbt project
        run: |
          tar -czf infrastructure/kubernetes/overlays/production/dbt-project.tar.gz \
            --exclude='venv' --exclude='target' --exclude='logs' \
            --exclude='dbt_packages' --exclude='__pycache__' --exclude='.user.yml' \
            -C tools/dbt .

      - name: Push Deploy Branch
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git checkout -B deploy
          git add infrastructure/kubernetes/overlays/production/
          git commit -m "deploy: resolve production values [skip ci]"
          git push --force origin deploy

      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig \
            --name ${{ steps.tf-outputs.outputs.cluster_name }} \
            --region us-east-1

      - name: Wait for EKS nodes to be ready
        run: |
          echo "Waiting for EKS nodes to reach Ready state..."
          kubectl wait --for=condition=Ready nodes --all --timeout=300s

      - name: Create Application Databases
        run: |
          echo "Creating application databases in RDS..."
          POSTGRES_PASSWORD=$(aws secretsmanager get-secret-value \
            --secret-id rideshare/rds --query SecretString --output text | jq -r '.MASTER_PASSWORD')
          DATA_PIPELINE_CREDS=$(aws secretsmanager get-secret-value \
            --secret-id rideshare/data-pipeline --query SecretString --output text)
          RDS_ENDPOINT="${{ steps.tf-outputs.outputs.rds_endpoint }}"

          # CREATE DATABASE cannot use IF NOT EXISTS in PostgreSQL.
          # Run it and treat "already exists" (exit code 1) as success.
          # Use PGPASSWORD env var instead of embedding password in URI to avoid
          # percent-encoding issues with special characters (%, >, etc.).
          for DB in metastore airflow; do
            kubectl run "rds-init-${DB}" --rm -i --restart=Never --namespace=default \
              --image=postgres:16 --env="PGPASSWORD=${POSTGRES_PASSWORD}" -- \
              psql -h "${RDS_ENDPOINT}" -p 5432 -U postgres -d postgres \
              -c "CREATE DATABASE ${DB};" \
              && echo "${DB} database created" \
              || echo "${DB} database may already exist (expected on re-runs)"
          done

          # Create metastore application user (used by Hive Metastore service)
          METASTORE_PASSWORD=$(echo "$DATA_PIPELINE_CREDS" | jq -r '.POSTGRES_METASTORE_PASSWORD')
          echo "CREATE ROLE metastore WITH LOGIN PASSWORD '${METASTORE_PASSWORD}'; GRANT ALL PRIVILEGES ON DATABASE metastore TO metastore; GRANT ALL PRIVILEGES ON SCHEMA public TO metastore;" | \
            kubectl run rds-create-metastore-user --rm -i --restart=Never --namespace=default \
              --image=postgres:16 --env="PGPASSWORD=${POSTGRES_PASSWORD}" -- \
              psql -h "${RDS_ENDPOINT}" -p 5432 -U postgres -d metastore \
            && echo "Metastore user created" \
            || echo "Metastore user may already exist (expected on re-runs)"

          # Create airflow application user (used by Airflow webserver/scheduler)
          AIRFLOW_PASSWORD=$(echo "$DATA_PIPELINE_CREDS" | jq -r '.POSTGRES_AIRFLOW_PASSWORD')
          echo "CREATE ROLE airflow WITH LOGIN PASSWORD '${AIRFLOW_PASSWORD}'; GRANT ALL PRIVILEGES ON DATABASE airflow TO airflow; GRANT ALL PRIVILEGES ON SCHEMA public TO airflow;" | \
            kubectl run rds-create-airflow-user --rm -i --restart=Never --namespace=default \
              --image=postgres:16 --env="PGPASSWORD=${POSTGRES_PASSWORD}" -- \
              psql -h "${RDS_ENDPOINT}" -p 5432 -U postgres -d airflow \
            && echo "Airflow user created" \
            || echo "Airflow user may already exist (expected on re-runs)"

          echo "Application databases ready"

      - name: Wait for ALB Controller or remove stale webhooks
        run: |
          echo "Waiting for ALB controller to be ready..."
          if ! kubectl wait --for=condition=available --timeout=120s \
            deployment/aws-load-balancer-controller -n kube-system 2>/dev/null; then
            echo "ALB controller not ready, removing stale webhooks to unblock deployments..."
            kubectl delete mutatingwebhookconfiguration aws-load-balancer-webhook 2>/dev/null || true
            kubectl delete validatingwebhookconfiguration aws-load-balancer-webhook 2>/dev/null || true
          fi

      - name: Install ArgoCD
        run: |
          # Check if ArgoCD is already installed
          if kubectl get namespace argocd &> /dev/null; then
            echo "ArgoCD namespace already exists, skipping installation"
          else
            echo "Installing ArgoCD..."
            kubectl create namespace argocd
            kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/v3.2.3/manifests/install.yaml
          fi

          # Wait for ArgoCD server to be ready
          echo "Waiting for ArgoCD server..."
          kubectl wait --for=condition=available --timeout=300s \
            deployment/argocd-server -n argocd

      - name: Configure ArgoCD kustomize build options
        run: |
          kubectl patch configmap argocd-cm -n argocd --type merge \
            -p '{"data":{"kustomize.buildOptions":"--load-restrictor=LoadRestrictionsNone"}}'

      - name: Apply ArgoCD Application
        run: |
          kubectl apply -f infrastructure/kubernetes/argocd/app-rideshare-platform.yaml

          echo "ArgoCD application created. Syncing..."
          kubectl get applications -n argocd

      - name: Health Checks
        run: |
          echo "Running health checks..."

          echo "Checking pods in rideshare-prod namespace..."
          kubectl get pods -n rideshare-prod

          echo "Waiting for simulation service..."
          kubectl wait --for=condition=available --timeout=600s \
            deployment/simulation -n rideshare-prod || echo "Simulation not ready yet"

          echo "Waiting for kafka statefulset..."
          kubectl wait --for=condition=ready --timeout=600s \
            pod/kafka-0 -n rideshare-prod || echo "Kafka not ready yet"

      - name: Get Load Balancer URL
        id: alb-url
        run: |
          echo "Waiting for ALB to be provisioned (up to 5 minutes)..."
          ALB_HOSTNAME=""
          for i in $(seq 1 30); do
            ALB_HOSTNAME=$(kubectl get ingress rideshare-api-ingress -n rideshare-prod \
              -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || true)
            if [ -n "$ALB_HOSTNAME" ]; then
              echo "ALB provisioned at: $ALB_HOSTNAME"
              break
            fi
            echo "Attempt $i/30: ALB not ready yet, waiting 10s..."
            sleep 10
          done

          if [ -z "$ALB_HOSTNAME" ]; then
            echo "Warning: ALB hostname not available after 5 minutes"
            ALB_HOSTNAME="pending"
          fi

          echo "alb_hostname=$ALB_HOSTNAME" >> "$GITHUB_OUTPUT"

      - name: Create Route 53 Wildcard Record
        if: steps.alb-url.outputs.alb_hostname != 'pending'
        run: |
          ALB_HOSTNAME="${{ steps.alb-url.outputs.alb_hostname }}"

          ZONE_ID=$(aws route53 list-hosted-zones-by-name \
            --dns-name ridesharing.portfolio.andresbrocco.com \
            --query 'HostedZones[0].Id' \
            --output text | sed 's|/hostedzone/||')

          if [ -z "$ZONE_ID" ]; then
            echo "Error: Could not find hosted zone"
            exit 1
          fi

          ALB_ZONE_ID=$(aws elbv2 describe-load-balancers \
            --query "LoadBalancers[?DNSName=='$ALB_HOSTNAME'].CanonicalHostedZoneId" \
            --output text)

          if [ -z "$ALB_ZONE_ID" ]; then
            echo "Error: Could not determine ALB hosted zone ID"
            exit 1
          fi

          echo "Creating Route 53 wildcard ALIAS record..."
          echo "  Zone ID: $ZONE_ID"
          echo "  ALB Hostname: $ALB_HOSTNAME"
          echo "  ALB Zone ID: $ALB_ZONE_ID"

          aws route53 change-resource-record-sets --hosted-zone-id "$ZONE_ID" \
            --change-batch "{
              \"Changes\": [{
                \"Action\": \"UPSERT\",
                \"ResourceRecordSet\": {
                  \"Name\": \"*.ridesharing.portfolio.andresbrocco.com\",
                  \"Type\": \"A\",
                  \"AliasTarget\": {
                    \"HostedZoneId\": \"$ALB_ZONE_ID\",
                    \"DNSName\": \"$ALB_HOSTNAME\",
                    \"EvaluateTargetHealth\": true
                  }
                }
              }]
            }"

          echo "Route 53 wildcard record created: *.ridesharing.portfolio.andresbrocco.com -> $ALB_HOSTNAME"

      - name: Deployment Summary
        run: |
          echo "================================"
          echo "Platform Deployment Complete"
          echo "================================"
          echo ""
          echo "EKS Cluster: ${{ steps.tf-outputs.outputs.cluster_name }}"
          echo "Cluster Endpoint: ${{ steps.tf-outputs.outputs.cluster_endpoint }}"
          echo "ALB Hostname: ${{ steps.alb-url.outputs.alb_hostname }}"
          echo ""
          echo "Landing Page:    https://ridesharing.portfolio.andresbrocco.com"
          echo "Control Panel:   https://control-panel.ridesharing.portfolio.andresbrocco.com"
          echo "API URL:         https://api.ridesharing.portfolio.andresbrocco.com"
          echo "Grafana:         https://grafana.ridesharing.portfolio.andresbrocco.com"
          echo "Airflow:      https://airflow.ridesharing.portfolio.andresbrocco.com"
          echo "Trino:        https://trino.ridesharing.portfolio.andresbrocco.com"
          echo "Prometheus:   https://prometheus.ridesharing.portfolio.andresbrocco.com"
          echo ""
          echo "Estimated cost: ~$0.65/hour while running"
          echo "Remember to run teardown workflow when demo is complete"
          echo "================================"

  deploy-frontend:
    if: contains(fromJSON('["deploy-all", "deploy-frontend"]'), github.event.inputs.action)
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: services/control-panel/package-lock.json

      - name: Install dependencies
        working-directory: services/control-panel
        run: npm ci

      - name: Build React app with production environment variables
        working-directory: services/control-panel
        env:
          VITE_API_URL: https://api.ridesharing.portfolio.andresbrocco.com
          VITE_WS_URL: wss://api.ridesharing.portfolio.andresbrocco.com/ws
          VITE_LAMBDA_URL: ${{ vars.LAMBDA_URL }}
        run: npm run build

      - name: Configure AWS credentials via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/rideshare-github-actions
          aws-region: us-east-1

      - name: Sync build output to S3
        working-directory: services/control-panel
        run: |
          BUCKET="rideshare-${{ secrets.AWS_ACCOUNT_ID }}-frontend"

          aws s3 sync dist/ "s3://${BUCKET}/" \
            --delete \
            --cache-control "public, max-age=31536000" \
            --exclude "index.html" \
            --exclude "*.html"

          aws s3 sync dist/ "s3://${BUCKET}/" \
            --exclude "*" \
            --include "*.html" \
            --cache-control "public, max-age=0, must-revalidate"

      - name: Invalidate CloudFront cache
        run: |
          DISTRIBUTION_ID=$(aws cloudfront list-distributions \
            --query "DistributionList.Items[?Comment=='Frontend SPA distribution'].Id" \
            --output text)

          if [ -z "$DISTRIBUTION_ID" ]; then
            echo "Error: CloudFront distribution not found"
            exit 1
          fi

          aws cloudfront create-invalidation \
            --distribution-id "$DISTRIBUTION_ID" \
            --paths "/*"

      - name: Output deployment summary
        run: |
          echo "Frontend deployed to S3 and CloudFront cache invalidated"
          echo "Landing Page:  https://ridesharing.portfolio.andresbrocco.com"
          echo "Control Panel: https://control-panel.ridesharing.portfolio.andresbrocco.com (via ALB/EKS)"
