groups:
  - name: prometheus_health
    interval: 30s
    rules:
      - alert: PrometheusDown
        expr: up{job="prometheus"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Prometheus instance down"
          description: "Prometheus has been down for more than 1 minute"

      - alert: PrometheusScrapeFailure
        expr: up == 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Target scrape failure"
          description: "Prometheus failed to scrape {{ $labels.job }} for 5 minutes"

  - name: container_health
    interval: 30s
    rules:
      - alert: HighContainerMemoryUsage
        expr: (container_memory_usage_bytes / container_spec_memory_limit_bytes) > 0.9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High container memory usage"
          description: "Container {{ $labels.name }} is using {{ $value | humanizePercentage }} of memory limit"

      - alert: ContainerDown
        expr: up{job="cadvisor"} == 0
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "cAdvisor down"
          description: "Container metrics collection is unavailable"

  - name: simulation_health
    interval: 30s
    rules:
      - alert: SimulationHighErrorRate
        expr: sum(rate(simulation_errors_total[5m])) > 10
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High simulation error rate"
          description: "Simulation is experiencing {{ $value | humanize }} errors per second"

      - alert: SimulationOSRMLatencyHigh
        expr: histogram_quantile(0.95, rate(simulation_osrm_latency_seconds_bucket[5m])) > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High OSRM latency"
          description: "OSRM p95 latency is {{ $value | humanizeDuration }}"

      - alert: StreamProcessorKafkaDisconnected
        expr: stream_processor_kafka_connected == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Stream processor lost Kafka connection"
          description: "Stream processor has been disconnected from Kafka for more than 1 minute"

      - alert: StreamProcessorRedisDisconnected
        expr: stream_processor_redis_connected == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Stream processor lost Redis connection"
          description: "Stream processor has been disconnected from Redis for more than 1 minute"

      - alert: SimulationQueueBacklog
        expr: simulation_simpy_events > 1000
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High SimPy event queue depth"
          description: "SimPy event queue has {{ $value }} pending events"

      - alert: SimulationServiceDown
        expr: absent_over_time(simulation_events_total[5m])
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Simulation service is not producing metrics"
          description: "No simulation_events_total metrics received for 5 minutes, indicating the simulation service may be down."

      - alert: StreamProcessorServiceDown
        expr: absent_over_time(stream_processor_messages_consumed_total[5m])
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Stream processor is not producing metrics"
          description: "No stream_processor_messages_consumed_total metrics received for 5 minutes, indicating the stream processor may be down."
